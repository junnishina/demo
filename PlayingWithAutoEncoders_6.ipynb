{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多層のdeepencoderでより少ない要素数で４と９を分離できるだろうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 300\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_train_idx = np.logical_or(y_train == 4, y_train == 9)\n",
    "keep_test_idx = np.logical_or(y_test ==4, y_test == 9)\n",
    "\n",
    "x_train = x_train[keep_train_idx]\n",
    "x_test = x_test[keep_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_vec = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test_vec = x_test.reshape(x_test.shape[0], 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中央の要素数は4個に設定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 404       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               500       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 784)               79184     \n",
      "=================================================================\n",
      "Total params: 158,588\n",
      "Trainable params: 158,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(784, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11791 samples, validate on 1991 samples\n",
      "Epoch 1/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0772 - acc: 0.0071 - val_loss: 0.0526 - val_acc: 0.0095\n",
      "Epoch 2/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0531 - acc: 0.0080 - val_loss: 0.0517 - val_acc: 0.0095\n",
      "Epoch 3/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0523 - acc: 0.0087 - val_loss: 0.0509 - val_acc: 0.0095\n",
      "Epoch 4/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0510 - acc: 0.0083 - val_loss: 0.0493 - val_acc: 0.0095\n",
      "Epoch 5/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0495 - acc: 0.0090 - val_loss: 0.0477 - val_acc: 0.0080\n",
      "Epoch 6/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0485 - acc: 0.0104 - val_loss: 0.0465 - val_acc: 0.0090\n",
      "Epoch 7/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0479 - acc: 0.0091 - val_loss: 0.0461 - val_acc: 0.0100\n",
      "Epoch 8/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0476 - acc: 0.0108 - val_loss: 0.0460 - val_acc: 0.0090\n",
      "Epoch 9/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0474 - acc: 0.0100 - val_loss: 0.0456 - val_acc: 0.0080\n",
      "Epoch 10/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0472 - acc: 0.0120 - val_loss: 0.0455 - val_acc: 0.0100\n",
      "Epoch 11/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0469 - acc: 0.0106 - val_loss: 0.0449 - val_acc: 0.0110\n",
      "Epoch 12/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0466 - acc: 0.0114 - val_loss: 0.0442 - val_acc: 0.0110\n",
      "Epoch 13/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0461 - acc: 0.0120 - val_loss: 0.0433 - val_acc: 0.0110\n",
      "Epoch 14/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0455 - acc: 0.0114 - val_loss: 0.0423 - val_acc: 0.0110\n",
      "Epoch 15/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0452 - acc: 0.0098 - val_loss: 0.0421 - val_acc: 0.0121\n",
      "Epoch 16/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0451 - acc: 0.0102 - val_loss: 0.0418 - val_acc: 0.0151\n",
      "Epoch 17/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0448 - acc: 0.0098 - val_loss: 0.0415 - val_acc: 0.0136\n",
      "Epoch 18/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0447 - acc: 0.0110 - val_loss: 0.0416 - val_acc: 0.0105\n",
      "Epoch 19/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0447 - acc: 0.0095 - val_loss: 0.0414 - val_acc: 0.0105\n",
      "Epoch 20/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0446 - acc: 0.0098 - val_loss: 0.0412 - val_acc: 0.0110\n",
      "Epoch 21/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0445 - acc: 0.0101 - val_loss: 0.0411 - val_acc: 0.0166\n",
      "Epoch 22/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0444 - acc: 0.0113 - val_loss: 0.0409 - val_acc: 0.0121\n",
      "Epoch 23/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0444 - acc: 0.0120 - val_loss: 0.0411 - val_acc: 0.0116\n",
      "Epoch 24/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0444 - acc: 0.0106 - val_loss: 0.0410 - val_acc: 0.0105\n",
      "Epoch 25/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0443 - acc: 0.0106 - val_loss: 0.0409 - val_acc: 0.0131\n",
      "Epoch 26/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0442 - acc: 0.0128 - val_loss: 0.0408 - val_acc: 0.0100\n",
      "Epoch 27/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0442 - acc: 0.0114 - val_loss: 0.0407 - val_acc: 0.0116\n",
      "Epoch 28/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0441 - acc: 0.0103 - val_loss: 0.0406 - val_acc: 0.0116\n",
      "Epoch 29/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0440 - acc: 0.0123 - val_loss: 0.0406 - val_acc: 0.0110\n",
      "Epoch 30/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0440 - acc: 0.0114 - val_loss: 0.0405 - val_acc: 0.0110\n",
      "Epoch 31/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0440 - acc: 0.0097 - val_loss: 0.0406 - val_acc: 0.0116\n",
      "Epoch 32/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0439 - acc: 0.0103 - val_loss: 0.0404 - val_acc: 0.0110\n",
      "Epoch 33/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0438 - acc: 0.0110 - val_loss: 0.0404 - val_acc: 0.0100\n",
      "Epoch 34/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0439 - acc: 0.0114 - val_loss: 0.0402 - val_acc: 0.0110\n",
      "Epoch 35/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0438 - acc: 0.0107 - val_loss: 0.0403 - val_acc: 0.0095\n",
      "Epoch 36/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0439 - acc: 0.0112 - val_loss: 0.0401 - val_acc: 0.0121\n",
      "Epoch 37/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0437 - acc: 0.0113 - val_loss: 0.0401 - val_acc: 0.0116\n",
      "Epoch 38/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0436 - acc: 0.0112 - val_loss: 0.0399 - val_acc: 0.0116\n",
      "Epoch 39/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0436 - acc: 0.0103 - val_loss: 0.0399 - val_acc: 0.0110\n",
      "Epoch 40/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0436 - acc: 0.0117 - val_loss: 0.0400 - val_acc: 0.0126\n",
      "Epoch 41/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0435 - acc: 0.0105 - val_loss: 0.0397 - val_acc: 0.0141\n",
      "Epoch 42/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0434 - acc: 0.0120 - val_loss: 0.0397 - val_acc: 0.0131\n",
      "Epoch 43/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0433 - acc: 0.0103 - val_loss: 0.0395 - val_acc: 0.0131\n",
      "Epoch 44/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0435 - acc: 0.0121 - val_loss: 0.0396 - val_acc: 0.0116\n",
      "Epoch 45/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0432 - acc: 0.0133 - val_loss: 0.0394 - val_acc: 0.0151\n",
      "Epoch 46/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0432 - acc: 0.0116 - val_loss: 0.0394 - val_acc: 0.0116\n",
      "Epoch 47/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0432 - acc: 0.0113 - val_loss: 0.0394 - val_acc: 0.0116\n",
      "Epoch 48/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0430 - acc: 0.0113 - val_loss: 0.0391 - val_acc: 0.0131\n",
      "Epoch 49/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0432 - acc: 0.0106 - val_loss: 0.0391 - val_acc: 0.0121\n",
      "Epoch 50/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0430 - acc: 0.0110 - val_loss: 0.0391 - val_acc: 0.0146\n",
      "Epoch 51/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0430 - acc: 0.0102 - val_loss: 0.0390 - val_acc: 0.0141\n",
      "Epoch 52/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0430 - acc: 0.0120 - val_loss: 0.0388 - val_acc: 0.0121\n",
      "Epoch 53/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0429 - acc: 0.0114 - val_loss: 0.0388 - val_acc: 0.0110\n",
      "Epoch 54/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0430 - acc: 0.0109 - val_loss: 0.0391 - val_acc: 0.0116\n",
      "Epoch 55/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0428 - acc: 0.0120 - val_loss: 0.0388 - val_acc: 0.0126\n",
      "Epoch 56/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0428 - acc: 0.0111 - val_loss: 0.0387 - val_acc: 0.0126\n",
      "Epoch 57/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0428 - acc: 0.0107 - val_loss: 0.0386 - val_acc: 0.0121\n",
      "Epoch 58/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0427 - acc: 0.0110 - val_loss: 0.0386 - val_acc: 0.0121\n",
      "Epoch 59/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0427 - acc: 0.0106 - val_loss: 0.0385 - val_acc: 0.0141\n",
      "Epoch 60/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0427 - acc: 0.0106 - val_loss: 0.0385 - val_acc: 0.0136\n",
      "Epoch 61/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0427 - acc: 0.0110 - val_loss: 0.0384 - val_acc: 0.0121\n",
      "Epoch 62/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0427 - acc: 0.0100 - val_loss: 0.0385 - val_acc: 0.0141\n",
      "Epoch 63/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0426 - acc: 0.0120 - val_loss: 0.0383 - val_acc: 0.0126\n",
      "Epoch 64/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 1s - loss: 0.0425 - acc: 0.0109 - val_loss: 0.0383 - val_acc: 0.0116\n",
      "Epoch 65/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0424 - acc: 0.0107 - val_loss: 0.0382 - val_acc: 0.0116\n",
      "Epoch 66/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0424 - acc: 0.0093 - val_loss: 0.0383 - val_acc: 0.0131\n",
      "Epoch 67/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0423 - acc: 0.0100 - val_loss: 0.0381 - val_acc: 0.0121\n",
      "Epoch 68/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0423 - acc: 0.0108 - val_loss: 0.0380 - val_acc: 0.0141\n",
      "Epoch 69/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0424 - acc: 0.0098 - val_loss: 0.0381 - val_acc: 0.0116\n",
      "Epoch 70/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0425 - acc: 0.0107 - val_loss: 0.0381 - val_acc: 0.0136\n",
      "Epoch 71/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0424 - acc: 0.0097 - val_loss: 0.0380 - val_acc: 0.0146\n",
      "Epoch 72/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0423 - acc: 0.0100 - val_loss: 0.0380 - val_acc: 0.0121\n",
      "Epoch 73/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0421 - acc: 0.0112 - val_loss: 0.0379 - val_acc: 0.0141\n",
      "Epoch 74/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0423 - acc: 0.0098 - val_loss: 0.0380 - val_acc: 0.0110\n",
      "Epoch 75/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0422 - acc: 0.0111 - val_loss: 0.0379 - val_acc: 0.0116\n",
      "Epoch 76/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0422 - acc: 0.0097 - val_loss: 0.0378 - val_acc: 0.0121\n",
      "Epoch 77/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0421 - acc: 0.0108 - val_loss: 0.0379 - val_acc: 0.0131\n",
      "Epoch 78/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0421 - acc: 0.0101 - val_loss: 0.0379 - val_acc: 0.0095\n",
      "Epoch 79/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0421 - acc: 0.0096 - val_loss: 0.0379 - val_acc: 0.0105\n",
      "Epoch 80/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0422 - acc: 0.0096 - val_loss: 0.0378 - val_acc: 0.0126\n",
      "Epoch 81/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0419 - acc: 0.0093 - val_loss: 0.0377 - val_acc: 0.0116\n",
      "Epoch 82/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0420 - acc: 0.0090 - val_loss: 0.0377 - val_acc: 0.0110\n",
      "Epoch 83/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0420 - acc: 0.0098 - val_loss: 0.0378 - val_acc: 0.0126\n",
      "Epoch 84/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0420 - acc: 0.0105 - val_loss: 0.0377 - val_acc: 0.0116\n",
      "Epoch 85/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0421 - acc: 0.0112 - val_loss: 0.0376 - val_acc: 0.0116\n",
      "Epoch 86/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0420 - acc: 0.0094 - val_loss: 0.0377 - val_acc: 0.0136\n",
      "Epoch 87/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0419 - acc: 0.0114 - val_loss: 0.0375 - val_acc: 0.0121\n",
      "Epoch 88/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0419 - acc: 0.0113 - val_loss: 0.0375 - val_acc: 0.0105\n",
      "Epoch 89/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0419 - acc: 0.0095 - val_loss: 0.0376 - val_acc: 0.0121\n",
      "Epoch 90/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0419 - acc: 0.0103 - val_loss: 0.0376 - val_acc: 0.0126\n",
      "Epoch 91/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0419 - acc: 0.0107 - val_loss: 0.0376 - val_acc: 0.0116\n",
      "Epoch 92/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0419 - acc: 0.0097 - val_loss: 0.0376 - val_acc: 0.0116\n",
      "Epoch 93/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0419 - acc: 0.0102 - val_loss: 0.0375 - val_acc: 0.0116\n",
      "Epoch 94/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0418 - acc: 0.0112 - val_loss: 0.0375 - val_acc: 0.0121\n",
      "Epoch 95/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0419 - acc: 0.0097 - val_loss: 0.0375 - val_acc: 0.0116\n",
      "Epoch 96/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0418 - acc: 0.0120 - val_loss: 0.0374 - val_acc: 0.0095\n",
      "Epoch 97/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0419 - acc: 0.0090 - val_loss: 0.0375 - val_acc: 0.0116\n",
      "Epoch 98/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0418 - acc: 0.0092 - val_loss: 0.0374 - val_acc: 0.0131\n",
      "Epoch 99/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0418 - acc: 0.0106 - val_loss: 0.0374 - val_acc: 0.0110\n",
      "Epoch 100/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0418 - acc: 0.0103 - val_loss: 0.0375 - val_acc: 0.0110\n",
      "Epoch 101/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0418 - acc: 0.0094 - val_loss: 0.0374 - val_acc: 0.0121\n",
      "Epoch 102/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0418 - acc: 0.0109 - val_loss: 0.0373 - val_acc: 0.0116\n",
      "Epoch 103/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0417 - acc: 0.0118 - val_loss: 0.0373 - val_acc: 0.0110\n",
      "Epoch 104/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0416 - acc: 0.0093 - val_loss: 0.0373 - val_acc: 0.0126\n",
      "Epoch 105/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0418 - acc: 0.0103 - val_loss: 0.0373 - val_acc: 0.0110\n",
      "Epoch 106/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0416 - acc: 0.0103 - val_loss: 0.0372 - val_acc: 0.0116\n",
      "Epoch 107/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0417 - acc: 0.0105 - val_loss: 0.0373 - val_acc: 0.0116\n",
      "Epoch 108/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0417 - acc: 0.0114 - val_loss: 0.0374 - val_acc: 0.0121\n",
      "Epoch 109/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0417 - acc: 0.0114 - val_loss: 0.0373 - val_acc: 0.0110\n",
      "Epoch 110/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0416 - acc: 0.0119 - val_loss: 0.0372 - val_acc: 0.0110\n",
      "Epoch 111/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0417 - acc: 0.0115 - val_loss: 0.0373 - val_acc: 0.0110\n",
      "Epoch 112/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0417 - acc: 0.0109 - val_loss: 0.0372 - val_acc: 0.0126\n",
      "Epoch 113/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0416 - acc: 0.0113 - val_loss: 0.0372 - val_acc: 0.0110\n",
      "Epoch 114/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0416 - acc: 0.0100 - val_loss: 0.0373 - val_acc: 0.0116\n",
      "Epoch 115/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0416 - acc: 0.0109 - val_loss: 0.0372 - val_acc: 0.0121\n",
      "Epoch 116/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0417 - acc: 0.0103 - val_loss: 0.0372 - val_acc: 0.0085\n",
      "Epoch 117/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0416 - acc: 0.0090 - val_loss: 0.0372 - val_acc: 0.0110\n",
      "Epoch 118/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0415 - acc: 0.0097 - val_loss: 0.0372 - val_acc: 0.0116\n",
      "Epoch 119/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0415 - acc: 0.0109 - val_loss: 0.0371 - val_acc: 0.0126\n",
      "Epoch 120/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0415 - acc: 0.0118 - val_loss: 0.0371 - val_acc: 0.0110\n",
      "Epoch 121/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0415 - acc: 0.0109 - val_loss: 0.0371 - val_acc: 0.0126\n",
      "Epoch 122/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0416 - acc: 0.0108 - val_loss: 0.0370 - val_acc: 0.0110\n",
      "Epoch 123/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0415 - acc: 0.0098 - val_loss: 0.0371 - val_acc: 0.0110\n",
      "Epoch 124/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0415 - acc: 0.0103 - val_loss: 0.0371 - val_acc: 0.0095\n",
      "Epoch 125/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0415 - acc: 0.0097 - val_loss: 0.0371 - val_acc: 0.0095\n",
      "Epoch 126/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0414 - acc: 0.0100 - val_loss: 0.0371 - val_acc: 0.0100\n",
      "Epoch 127/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0413 - acc: 0.0106 - val_loss: 0.0369 - val_acc: 0.0100\n",
      "Epoch 128/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0415 - acc: 0.0095 - val_loss: 0.0370 - val_acc: 0.0110\n",
      "Epoch 129/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0415 - acc: 0.0103 - val_loss: 0.0371 - val_acc: 0.0110\n",
      "Epoch 130/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0414 - acc: 0.0112 - val_loss: 0.0370 - val_acc: 0.0121\n",
      "Epoch 131/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0414 - acc: 0.0109 - val_loss: 0.0370 - val_acc: 0.0110\n",
      "Epoch 132/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0415 - acc: 0.0100 - val_loss: 0.0370 - val_acc: 0.0095\n",
      "Epoch 133/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0415 - acc: 0.0105 - val_loss: 0.0370 - val_acc: 0.0110\n",
      "Epoch 134/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0414 - acc: 0.0096 - val_loss: 0.0370 - val_acc: 0.0105\n",
      "Epoch 135/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0414 - acc: 0.0111 - val_loss: 0.0370 - val_acc: 0.0121\n",
      "Epoch 136/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0414 - acc: 0.0111 - val_loss: 0.0370 - val_acc: 0.0121\n",
      "Epoch 137/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0414 - acc: 0.0103 - val_loss: 0.0369 - val_acc: 0.0126\n",
      "Epoch 138/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0413 - acc: 0.0118 - val_loss: 0.0369 - val_acc: 0.0090\n",
      "Epoch 139/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0413 - acc: 0.0118 - val_loss: 0.0368 - val_acc: 0.0105\n",
      "Epoch 140/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0414 - acc: 0.0123 - val_loss: 0.0368 - val_acc: 0.0110\n",
      "Epoch 141/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0413 - acc: 0.0122 - val_loss: 0.0369 - val_acc: 0.0116\n",
      "Epoch 142/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0414 - acc: 0.0108 - val_loss: 0.0369 - val_acc: 0.0121\n",
      "Epoch 143/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0412 - acc: 0.0129 - val_loss: 0.0368 - val_acc: 0.0116\n",
      "Epoch 144/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0413 - acc: 0.0102 - val_loss: 0.0368 - val_acc: 0.0116\n",
      "Epoch 145/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0412 - acc: 0.0107 - val_loss: 0.0368 - val_acc: 0.0110\n",
      "Epoch 146/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0412 - acc: 0.0114 - val_loss: 0.0368 - val_acc: 0.0131\n",
      "Epoch 147/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0410 - acc: 0.0107 - val_loss: 0.0367 - val_acc: 0.0121\n",
      "Epoch 148/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0413 - acc: 0.0119 - val_loss: 0.0369 - val_acc: 0.0121\n",
      "Epoch 149/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0412 - acc: 0.0109 - val_loss: 0.0367 - val_acc: 0.0116\n",
      "Epoch 150/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0411 - acc: 0.0102 - val_loss: 0.0367 - val_acc: 0.0110\n",
      "Epoch 151/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0412 - acc: 0.0109 - val_loss: 0.0369 - val_acc: 0.0121\n",
      "Epoch 152/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0411 - acc: 0.0120 - val_loss: 0.0368 - val_acc: 0.0105\n",
      "Epoch 153/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0411 - acc: 0.0113 - val_loss: 0.0366 - val_acc: 0.0121\n",
      "Epoch 154/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0410 - acc: 0.0118 - val_loss: 0.0366 - val_acc: 0.0110\n",
      "Epoch 155/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0411 - acc: 0.0113 - val_loss: 0.0367 - val_acc: 0.0131\n",
      "Epoch 156/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0411 - acc: 0.0110 - val_loss: 0.0366 - val_acc: 0.0116\n",
      "Epoch 157/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0411 - acc: 0.0108 - val_loss: 0.0366 - val_acc: 0.0110\n",
      "Epoch 158/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0411 - acc: 0.0096 - val_loss: 0.0366 - val_acc: 0.0110\n",
      "Epoch 159/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0409 - acc: 0.0111 - val_loss: 0.0366 - val_acc: 0.0121\n",
      "Epoch 160/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0410 - acc: 0.0103 - val_loss: 0.0365 - val_acc: 0.0116\n",
      "Epoch 161/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0410 - acc: 0.0103 - val_loss: 0.0366 - val_acc: 0.0110\n",
      "Epoch 162/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0409 - acc: 0.0103 - val_loss: 0.0366 - val_acc: 0.0110\n",
      "Epoch 163/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0411 - acc: 0.0103 - val_loss: 0.0367 - val_acc: 0.0126\n",
      "Epoch 164/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0410 - acc: 0.0126 - val_loss: 0.0366 - val_acc: 0.0105\n",
      "Epoch 165/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0411 - acc: 0.0126 - val_loss: 0.0365 - val_acc: 0.0116\n",
      "Epoch 166/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0410 - acc: 0.0103 - val_loss: 0.0365 - val_acc: 0.0100\n",
      "Epoch 167/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0410 - acc: 0.0111 - val_loss: 0.0365 - val_acc: 0.0105\n",
      "Epoch 168/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0409 - acc: 0.0108 - val_loss: 0.0365 - val_acc: 0.0110\n",
      "Epoch 169/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0409 - acc: 0.0098 - val_loss: 0.0365 - val_acc: 0.0100\n",
      "Epoch 170/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0410 - acc: 0.0098 - val_loss: 0.0365 - val_acc: 0.0110\n",
      "Epoch 171/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0409 - acc: 0.0103 - val_loss: 0.0364 - val_acc: 0.0110\n",
      "Epoch 172/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0409 - acc: 0.0117 - val_loss: 0.0365 - val_acc: 0.0121\n",
      "Epoch 173/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0408 - acc: 0.0106 - val_loss: 0.0365 - val_acc: 0.0110\n",
      "Epoch 174/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0407 - acc: 0.0113 - val_loss: 0.0364 - val_acc: 0.0090\n",
      "Epoch 175/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0409 - acc: 0.0103 - val_loss: 0.0366 - val_acc: 0.0105\n",
      "Epoch 176/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0409 - acc: 0.0111 - val_loss: 0.0364 - val_acc: 0.0121\n",
      "Epoch 177/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0408 - acc: 0.0100 - val_loss: 0.0364 - val_acc: 0.0100\n",
      "Epoch 178/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0409 - acc: 0.0102 - val_loss: 0.0365 - val_acc: 0.0121\n",
      "Epoch 179/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0408 - acc: 0.0110 - val_loss: 0.0364 - val_acc: 0.0095\n",
      "Epoch 180/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0407 - acc: 0.0126 - val_loss: 0.0364 - val_acc: 0.0100\n",
      "Epoch 181/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0408 - acc: 0.0106 - val_loss: 0.0363 - val_acc: 0.0105\n",
      "Epoch 182/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0408 - acc: 0.0114 - val_loss: 0.0363 - val_acc: 0.0070\n",
      "Epoch 183/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0408 - acc: 0.0089 - val_loss: 0.0364 - val_acc: 0.0110\n",
      "Epoch 184/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0408 - acc: 0.0112 - val_loss: 0.0364 - val_acc: 0.0085\n",
      "Epoch 185/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0103 - val_loss: 0.0363 - val_acc: 0.0090\n",
      "Epoch 186/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0408 - acc: 0.0104 - val_loss: 0.0364 - val_acc: 0.0085\n",
      "Epoch 187/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0407 - acc: 0.0114 - val_loss: 0.0363 - val_acc: 0.0090\n",
      "Epoch 188/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0095 - val_loss: 0.0363 - val_acc: 0.0090\n",
      "Epoch 189/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0100 - val_loss: 0.0364 - val_acc: 0.0100\n",
      "Epoch 190/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0407 - acc: 0.0109 - val_loss: 0.0363 - val_acc: 0.0126\n",
      "Epoch 191/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0107 - val_loss: 0.0363 - val_acc: 0.0085\n",
      "Epoch 192/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0090 - val_loss: 0.0362 - val_acc: 0.0110\n",
      "Epoch 193/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0104 - val_loss: 0.0363 - val_acc: 0.0090\n",
      "Epoch 194/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0091 - val_loss: 0.0363 - val_acc: 0.0100\n",
      "Epoch 195/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0405 - acc: 0.0113 - val_loss: 0.0362 - val_acc: 0.0116\n",
      "Epoch 196/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0406 - acc: 0.0115 - val_loss: 0.0363 - val_acc: 0.0100\n",
      "Epoch 197/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0119 - val_loss: 0.0363 - val_acc: 0.0090\n",
      "Epoch 198/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0407 - acc: 0.0101 - val_loss: 0.0364 - val_acc: 0.0090\n",
      "Epoch 199/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0087 - val_loss: 0.0364 - val_acc: 0.0090\n",
      "Epoch 200/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0407 - acc: 0.0103 - val_loss: 0.0364 - val_acc: 0.0116\n",
      "Epoch 201/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0406 - acc: 0.0108 - val_loss: 0.0363 - val_acc: 0.0105\n",
      "Epoch 202/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0404 - acc: 0.0087 - val_loss: 0.0364 - val_acc: 0.0110\n",
      "Epoch 203/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0407 - acc: 0.0102 - val_loss: 0.0364 - val_acc: 0.0070\n",
      "Epoch 204/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0406 - acc: 0.0101 - val_loss: 0.0364 - val_acc: 0.0095\n",
      "Epoch 205/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0105 - val_loss: 0.0363 - val_acc: 0.0080\n",
      "Epoch 206/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0104 - val_loss: 0.0363 - val_acc: 0.0095\n",
      "Epoch 207/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0103 - val_loss: 0.0363 - val_acc: 0.0105\n",
      "Epoch 208/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0109 - val_loss: 0.0364 - val_acc: 0.0085\n",
      "Epoch 209/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0405 - acc: 0.0107 - val_loss: 0.0362 - val_acc: 0.0085\n",
      "Epoch 210/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0406 - acc: 0.0103 - val_loss: 0.0363 - val_acc: 0.0090\n",
      "Epoch 211/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0405 - acc: 0.0115 - val_loss: 0.0363 - val_acc: 0.0070\n",
      "Epoch 212/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0405 - acc: 0.0094 - val_loss: 0.0362 - val_acc: 0.0090\n",
      "Epoch 213/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0404 - acc: 0.0103 - val_loss: 0.0363 - val_acc: 0.0070\n",
      "Epoch 214/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0405 - acc: 0.0109 - val_loss: 0.0363 - val_acc: 0.0095\n",
      "Epoch 215/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0406 - acc: 0.0105 - val_loss: 0.0364 - val_acc: 0.0080\n",
      "Epoch 216/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0404 - acc: 0.0100 - val_loss: 0.0362 - val_acc: 0.0085\n",
      "Epoch 217/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0405 - acc: 0.0092 - val_loss: 0.0362 - val_acc: 0.0090\n",
      "Epoch 218/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0405 - acc: 0.0106 - val_loss: 0.0363 - val_acc: 0.0095\n",
      "Epoch 219/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0405 - acc: 0.0103 - val_loss: 0.0364 - val_acc: 0.0090\n",
      "Epoch 220/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0406 - acc: 0.0107 - val_loss: 0.0363 - val_acc: 0.0090\n",
      "Epoch 221/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0405 - acc: 0.0100 - val_loss: 0.0363 - val_acc: 0.0110\n",
      "Epoch 222/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0405 - acc: 0.0109 - val_loss: 0.0363 - val_acc: 0.0116\n",
      "Epoch 223/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0404 - acc: 0.0113 - val_loss: 0.0362 - val_acc: 0.0095\n",
      "Epoch 224/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0405 - acc: 0.0115 - val_loss: 0.0363 - val_acc: 0.0090\n",
      "Epoch 225/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0404 - acc: 0.0108 - val_loss: 0.0363 - val_acc: 0.0080\n",
      "Epoch 226/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0404 - acc: 0.0112 - val_loss: 0.0362 - val_acc: 0.0110\n",
      "Epoch 227/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0403 - acc: 0.0105 - val_loss: 0.0362 - val_acc: 0.0085\n",
      "Epoch 228/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0404 - acc: 0.0103 - val_loss: 0.0362 - val_acc: 0.0090\n",
      "Epoch 229/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0403 - acc: 0.0109 - val_loss: 0.0363 - val_acc: 0.0085\n",
      "Epoch 230/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0403 - acc: 0.0101 - val_loss: 0.0363 - val_acc: 0.0070\n",
      "Epoch 231/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0404 - acc: 0.0091 - val_loss: 0.0364 - val_acc: 0.0080\n",
      "Epoch 232/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0404 - acc: 0.0103 - val_loss: 0.0364 - val_acc: 0.0075\n",
      "Epoch 233/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0404 - acc: 0.0095 - val_loss: 0.0363 - val_acc: 0.0075\n",
      "Epoch 234/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0404 - acc: 0.0117 - val_loss: 0.0363 - val_acc: 0.0095\n",
      "Epoch 235/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0404 - acc: 0.0097 - val_loss: 0.0363 - val_acc: 0.0080\n",
      "Epoch 236/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0403 - acc: 0.0103 - val_loss: 0.0362 - val_acc: 0.0095\n",
      "Epoch 237/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0402 - acc: 0.0094 - val_loss: 0.0363 - val_acc: 0.0080\n",
      "Epoch 238/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0089 - val_loss: 0.0362 - val_acc: 0.0090\n",
      "Epoch 239/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0403 - acc: 0.0088 - val_loss: 0.0363 - val_acc: 0.0095\n",
      "Epoch 240/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0403 - acc: 0.0094 - val_loss: 0.0362 - val_acc: 0.0085\n",
      "Epoch 241/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0403 - acc: 0.0101 - val_loss: 0.0362 - val_acc: 0.0095\n",
      "Epoch 242/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0404 - acc: 0.0103 - val_loss: 0.0363 - val_acc: 0.0095\n",
      "Epoch 243/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0404 - acc: 0.0100 - val_loss: 0.0363 - val_acc: 0.0085\n",
      "Epoch 244/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0404 - acc: 0.0106 - val_loss: 0.0363 - val_acc: 0.0095\n",
      "Epoch 245/300\n",
      "11791/11791 [==============================] - ETA: 0s - loss: 0.0403 - acc: 0.010 - 1s - loss: 0.0403 - acc: 0.0102 - val_loss: 0.0363 - val_acc: 0.0080\n",
      "Epoch 246/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0403 - acc: 0.0114 - val_loss: 0.0364 - val_acc: 0.0085\n",
      "Epoch 247/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0103 - val_loss: 0.0363 - val_acc: 0.0085\n",
      "Epoch 248/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0095 - val_loss: 0.0364 - val_acc: 0.0075\n",
      "Epoch 249/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0403 - acc: 0.0093 - val_loss: 0.0363 - val_acc: 0.0090\n",
      "Epoch 250/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0114 - val_loss: 0.0363 - val_acc: 0.0095\n",
      "Epoch 251/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0104 - val_loss: 0.0362 - val_acc: 0.0085\n",
      "Epoch 252/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 1s - loss: 0.0404 - acc: 0.0094 - val_loss: 0.0363 - val_acc: 0.0105\n",
      "Epoch 253/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0102 - val_loss: 0.0364 - val_acc: 0.0090\n",
      "Epoch 254/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0403 - acc: 0.0095 - val_loss: 0.0363 - val_acc: 0.0060\n",
      "Epoch 255/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0087 - val_loss: 0.0363 - val_acc: 0.0090\n",
      "Epoch 256/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0103 - val_loss: 0.0363 - val_acc: 0.0085\n",
      "Epoch 257/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0109 - val_loss: 0.0363 - val_acc: 0.0075\n",
      "Epoch 258/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0403 - acc: 0.0097 - val_loss: 0.0362 - val_acc: 0.0085\n",
      "Epoch 259/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0105 - val_loss: 0.0363 - val_acc: 0.0065\n",
      "Epoch 260/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0401 - acc: 0.0104 - val_loss: 0.0363 - val_acc: 0.0085\n",
      "Epoch 261/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0403 - acc: 0.0105 - val_loss: 0.0364 - val_acc: 0.0090\n",
      "Epoch 262/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0401 - acc: 0.0103 - val_loss: 0.0363 - val_acc: 0.0085\n",
      "Epoch 263/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0087 - val_loss: 0.0363 - val_acc: 0.0085\n",
      "Epoch 264/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0094 - val_loss: 0.0363 - val_acc: 0.0080\n",
      "Epoch 265/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0403 - acc: 0.0104 - val_loss: 0.0363 - val_acc: 0.0075\n",
      "Epoch 266/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0109 - val_loss: 0.0365 - val_acc: 0.0080\n",
      "Epoch 267/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0402 - acc: 0.0093 - val_loss: 0.0362 - val_acc: 0.0090\n",
      "Epoch 268/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0103 - val_loss: 0.0363 - val_acc: 0.0090\n",
      "Epoch 269/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0400 - acc: 0.0106 - val_loss: 0.0364 - val_acc: 0.0100\n",
      "Epoch 270/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0088 - val_loss: 0.0365 - val_acc: 0.0080\n",
      "Epoch 271/300\n",
      "11791/11791 [==============================] - ETA: 0s - loss: 0.0401 - acc: 0.010 - 1s - loss: 0.0401 - acc: 0.0106 - val_loss: 0.0363 - val_acc: 0.0080\n",
      "Epoch 272/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0096 - val_loss: 0.0363 - val_acc: 0.0080\n",
      "Epoch 273/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0087 - val_loss: 0.0363 - val_acc: 0.0085\n",
      "Epoch 274/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0402 - acc: 0.0100 - val_loss: 0.0365 - val_acc: 0.0080\n",
      "Epoch 275/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0098 - val_loss: 0.0363 - val_acc: 0.0080\n",
      "Epoch 276/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0096 - val_loss: 0.0363 - val_acc: 0.0090\n",
      "Epoch 277/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0400 - acc: 0.0084 - val_loss: 0.0364 - val_acc: 0.0085\n",
      "Epoch 278/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0086 - val_loss: 0.0364 - val_acc: 0.0070\n",
      "Epoch 279/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0104 - val_loss: 0.0364 - val_acc: 0.0080\n",
      "Epoch 280/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0099 - val_loss: 0.0364 - val_acc: 0.0105\n",
      "Epoch 281/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0400 - acc: 0.0098 - val_loss: 0.0364 - val_acc: 0.0075\n",
      "Epoch 282/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0400 - acc: 0.0100 - val_loss: 0.0363 - val_acc: 0.0100\n",
      "Epoch 283/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0108 - val_loss: 0.0363 - val_acc: 0.0100\n",
      "Epoch 284/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0399 - acc: 0.0103 - val_loss: 0.0364 - val_acc: 0.0070\n",
      "Epoch 285/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0098 - val_loss: 0.0363 - val_acc: 0.0060\n",
      "Epoch 286/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0098 - val_loss: 0.0363 - val_acc: 0.0070\n",
      "Epoch 287/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0107 - val_loss: 0.0363 - val_acc: 0.0090\n",
      "Epoch 288/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0103 - val_loss: 0.0364 - val_acc: 0.0090\n",
      "Epoch 289/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0400 - acc: 0.0104 - val_loss: 0.0362 - val_acc: 0.0085\n",
      "Epoch 290/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0400 - acc: 0.0111 - val_loss: 0.0362 - val_acc: 0.0090\n",
      "Epoch 291/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0400 - acc: 0.0104 - val_loss: 0.0364 - val_acc: 0.0090\n",
      "Epoch 292/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0400 - acc: 0.0101 - val_loss: 0.0363 - val_acc: 0.0080\n",
      "Epoch 293/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0400 - acc: 0.0095 - val_loss: 0.0364 - val_acc: 0.0070\n",
      "Epoch 294/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0399 - acc: 0.0098 - val_loss: 0.0364 - val_acc: 0.0075\n",
      "Epoch 295/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0400 - acc: 0.0101 - val_loss: 0.0362 - val_acc: 0.0095\n",
      "Epoch 296/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0092 - val_loss: 0.0362 - val_acc: 0.0100\n",
      "Epoch 297/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0401 - acc: 0.0095 - val_loss: 0.0363 - val_acc: 0.0085\n",
      "Epoch 298/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0400 - acc: 0.0094 - val_loss: 0.0364 - val_acc: 0.0095\n",
      "Epoch 299/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0399 - acc: 0.0080 - val_loss: 0.0362 - val_acc: 0.0080\n",
      "Epoch 300/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0400 - acc: 0.0100 - val_loss: 0.0363 - val_acc: 0.0070\n",
      "Test loss: 0.0363182730576\n",
      "Test accuracy: 0.00703164239076\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_vec, x_train_vec,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_vec, x_test_vec))\n",
    "score = model.evaluate(x_test_vec, x_test_vec, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エポック数は先ほどより長くしたこともあり少々待ち時間がかかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22fafa4d5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    predicted = model.predict(x_train_vec[i:i+1]).reshape((28,28))\n",
    "    plt.imshow(predicted)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "４については９になってしまっているものもある。<br>要素数の最適化のかわりに、スパース正則化というテクニックがある。これはkerasのregularizerを指定するだけでできる。これはPCAと同じなのだが、線形な分離しかできない。最近の流行は<a href=\"http://cs.stanford.edu/people/karpathy/tsnejs/\">tSNEアルゴリズム</a>である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
