{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多層のdeepencoderを試みる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 300\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_train_idx = np.logical_or(y_train == 4, y_train == 9)\n",
    "keep_test_idx = np.logical_or(y_test ==4, y_test == 9)\n",
    "\n",
    "x_train = x_train[keep_train_idx]\n",
    "x_test = x_test[keep_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_vec = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test_vec = x_test.reshape(x_test.shape[0], 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中央の要素数は先ほどと同じ12個に設定する。層間の比をだいたい同じにすると良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 12)                1212      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               1300      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 784)               79184     \n",
      "=================================================================\n",
      "Total params: 160,196\n",
      "Trainable params: 160,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(12, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(784, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11791 samples, validate on 1991 samples\n",
      "Epoch 1/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0751 - acc: 0.0080 - val_loss: 0.0524 - val_acc: 0.0015\n",
      "Epoch 2/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0530 - acc: 0.0081 - val_loss: 0.0517 - val_acc: 0.0095\n",
      "Epoch 3/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0515 - acc: 0.0084 - val_loss: 0.0493 - val_acc: 0.0040\n",
      "Epoch 4/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0487 - acc: 0.0102 - val_loss: 0.0465 - val_acc: 0.0090\n",
      "Epoch 5/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0471 - acc: 0.0109 - val_loss: 0.0455 - val_acc: 0.0105\n",
      "Epoch 6/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0463 - acc: 0.0100 - val_loss: 0.0443 - val_acc: 0.0075\n",
      "Epoch 7/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0451 - acc: 0.0102 - val_loss: 0.0424 - val_acc: 0.0095\n",
      "Epoch 8/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0436 - acc: 0.0123 - val_loss: 0.0409 - val_acc: 0.0080\n",
      "Epoch 9/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0428 - acc: 0.0109 - val_loss: 0.0403 - val_acc: 0.0110\n",
      "Epoch 10/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0424 - acc: 0.0107 - val_loss: 0.0399 - val_acc: 0.0121\n",
      "Epoch 11/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0421 - acc: 0.0126 - val_loss: 0.0395 - val_acc: 0.0136\n",
      "Epoch 12/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0418 - acc: 0.0112 - val_loss: 0.0392 - val_acc: 0.0090\n",
      "Epoch 13/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0415 - acc: 0.0109 - val_loss: 0.0387 - val_acc: 0.0126\n",
      "Epoch 14/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0412 - acc: 0.0113 - val_loss: 0.0384 - val_acc: 0.0126\n",
      "Epoch 15/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0409 - acc: 0.0111 - val_loss: 0.0378 - val_acc: 0.0121\n",
      "Epoch 16/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0406 - acc: 0.0102 - val_loss: 0.0375 - val_acc: 0.0126\n",
      "Epoch 17/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0404 - acc: 0.0104 - val_loss: 0.0372 - val_acc: 0.0136\n",
      "Epoch 18/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0402 - acc: 0.0100 - val_loss: 0.0370 - val_acc: 0.0121\n",
      "Epoch 19/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0400 - acc: 0.0102 - val_loss: 0.0367 - val_acc: 0.0131\n",
      "Epoch 20/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0398 - acc: 0.0103 - val_loss: 0.0367 - val_acc: 0.0085\n",
      "Epoch 21/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0396 - acc: 0.0095 - val_loss: 0.0363 - val_acc: 0.0116\n",
      "Epoch 22/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0394 - acc: 0.0084 - val_loss: 0.0362 - val_acc: 0.0100\n",
      "Epoch 23/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0394 - acc: 0.0092 - val_loss: 0.0360 - val_acc: 0.0121\n",
      "Epoch 24/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0392 - acc: 0.0101 - val_loss: 0.0359 - val_acc: 0.0095\n",
      "Epoch 25/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0390 - acc: 0.0094 - val_loss: 0.0357 - val_acc: 0.0100\n",
      "Epoch 26/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0389 - acc: 0.0093 - val_loss: 0.0356 - val_acc: 0.0116\n",
      "Epoch 27/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0388 - acc: 0.0088 - val_loss: 0.0355 - val_acc: 0.0105\n",
      "Epoch 28/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0387 - acc: 0.0099 - val_loss: 0.0353 - val_acc: 0.0141\n",
      "Epoch 29/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0385 - acc: 0.0096 - val_loss: 0.0351 - val_acc: 0.0126\n",
      "Epoch 30/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0385 - acc: 0.0098 - val_loss: 0.0351 - val_acc: 0.0141\n",
      "Epoch 31/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0384 - acc: 0.0093 - val_loss: 0.0350 - val_acc: 0.0121\n",
      "Epoch 32/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0382 - acc: 0.0093 - val_loss: 0.0348 - val_acc: 0.0116\n",
      "Epoch 33/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0382 - acc: 0.0085 - val_loss: 0.0347 - val_acc: 0.0110\n",
      "Epoch 34/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0381 - acc: 0.0094 - val_loss: 0.0346 - val_acc: 0.0100\n",
      "Epoch 35/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0380 - acc: 0.0096 - val_loss: 0.0344 - val_acc: 0.0100\n",
      "Epoch 36/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0378 - acc: 0.0088 - val_loss: 0.0343 - val_acc: 0.0110\n",
      "Epoch 37/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0378 - acc: 0.0092 - val_loss: 0.0342 - val_acc: 0.0095\n",
      "Epoch 38/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0377 - acc: 0.0096 - val_loss: 0.0341 - val_acc: 0.0105\n",
      "Epoch 39/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0377 - acc: 0.0083 - val_loss: 0.0339 - val_acc: 0.0090\n",
      "Epoch 40/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0375 - acc: 0.0083 - val_loss: 0.0338 - val_acc: 0.0080\n",
      "Epoch 41/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0375 - acc: 0.0079 - val_loss: 0.0337 - val_acc: 0.0080\n",
      "Epoch 42/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0375 - acc: 0.0085 - val_loss: 0.0337 - val_acc: 0.0110\n",
      "Epoch 43/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0373 - acc: 0.0087 - val_loss: 0.0335 - val_acc: 0.0116\n",
      "Epoch 44/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0372 - acc: 0.0085 - val_loss: 0.0333 - val_acc: 0.0095\n",
      "Epoch 45/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0371 - acc: 0.0091 - val_loss: 0.0334 - val_acc: 0.0110\n",
      "Epoch 46/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0370 - acc: 0.0100 - val_loss: 0.0332 - val_acc: 0.0100\n",
      "Epoch 47/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0371 - acc: 0.0087 - val_loss: 0.0331 - val_acc: 0.0095\n",
      "Epoch 48/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0370 - acc: 0.0081 - val_loss: 0.0330 - val_acc: 0.0105\n",
      "Epoch 49/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0369 - acc: 0.0081 - val_loss: 0.0329 - val_acc: 0.0105\n",
      "Epoch 50/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0368 - acc: 0.0104 - val_loss: 0.0327 - val_acc: 0.0100\n",
      "Epoch 51/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0368 - acc: 0.0096 - val_loss: 0.0327 - val_acc: 0.0110\n",
      "Epoch 52/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0367 - acc: 0.0098 - val_loss: 0.0325 - val_acc: 0.0110\n",
      "Epoch 53/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0367 - acc: 0.0084 - val_loss: 0.0325 - val_acc: 0.0095\n",
      "Epoch 54/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0366 - acc: 0.0091 - val_loss: 0.0324 - val_acc: 0.0105\n",
      "Epoch 55/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0366 - acc: 0.0087 - val_loss: 0.0323 - val_acc: 0.0085\n",
      "Epoch 56/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0364 - acc: 0.0084 - val_loss: 0.0322 - val_acc: 0.0100\n",
      "Epoch 57/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0364 - acc: 0.0094 - val_loss: 0.0322 - val_acc: 0.0090\n",
      "Epoch 58/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0364 - acc: 0.0076 - val_loss: 0.0321 - val_acc: 0.0110\n",
      "Epoch 59/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0364 - acc: 0.0075 - val_loss: 0.0322 - val_acc: 0.0105\n",
      "Epoch 60/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0363 - acc: 0.0083 - val_loss: 0.0320 - val_acc: 0.0116\n",
      "Epoch 61/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0362 - acc: 0.0101 - val_loss: 0.0320 - val_acc: 0.0116\n",
      "Epoch 62/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0362 - acc: 0.0087 - val_loss: 0.0319 - val_acc: 0.0100\n",
      "Epoch 63/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0362 - acc: 0.0095 - val_loss: 0.0317 - val_acc: 0.0121\n",
      "Epoch 64/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0361 - acc: 0.0081 - val_loss: 0.0317 - val_acc: 0.0126\n",
      "Epoch 65/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0362 - acc: 0.0092 - val_loss: 0.0316 - val_acc: 0.0100\n",
      "Epoch 66/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0360 - acc: 0.0090 - val_loss: 0.0315 - val_acc: 0.0110\n",
      "Epoch 67/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0361 - acc: 0.0092 - val_loss: 0.0315 - val_acc: 0.0126\n",
      "Epoch 68/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0360 - acc: 0.0089 - val_loss: 0.0314 - val_acc: 0.0121\n",
      "Epoch 69/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0359 - acc: 0.0089 - val_loss: 0.0313 - val_acc: 0.0100\n",
      "Epoch 70/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0359 - acc: 0.0095 - val_loss: 0.0312 - val_acc: 0.0121\n",
      "Epoch 71/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0358 - acc: 0.0082 - val_loss: 0.0312 - val_acc: 0.0105\n",
      "Epoch 72/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0357 - acc: 0.0086 - val_loss: 0.0311 - val_acc: 0.0105\n",
      "Epoch 73/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0357 - acc: 0.0092 - val_loss: 0.0310 - val_acc: 0.0110\n",
      "Epoch 74/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0357 - acc: 0.0084 - val_loss: 0.0310 - val_acc: 0.0110\n",
      "Epoch 75/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0357 - acc: 0.0070 - val_loss: 0.0308 - val_acc: 0.0105\n",
      "Epoch 76/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0356 - acc: 0.0096 - val_loss: 0.0308 - val_acc: 0.0100\n",
      "Epoch 77/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0355 - acc: 0.0092 - val_loss: 0.0308 - val_acc: 0.0105\n",
      "Epoch 78/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0355 - acc: 0.0094 - val_loss: 0.0306 - val_acc: 0.0116\n",
      "Epoch 79/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0355 - acc: 0.0103 - val_loss: 0.0307 - val_acc: 0.0110\n",
      "Epoch 80/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0355 - acc: 0.0098 - val_loss: 0.0306 - val_acc: 0.0100\n",
      "Epoch 81/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0354 - acc: 0.0087 - val_loss: 0.0305 - val_acc: 0.0095\n",
      "Epoch 82/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0353 - acc: 0.0098 - val_loss: 0.0304 - val_acc: 0.0126\n",
      "Epoch 83/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0353 - acc: 0.0080 - val_loss: 0.0304 - val_acc: 0.0121\n",
      "Epoch 84/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0353 - acc: 0.0099 - val_loss: 0.0301 - val_acc: 0.0110\n",
      "Epoch 85/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0353 - acc: 0.0103 - val_loss: 0.0301 - val_acc: 0.0110\n",
      "Epoch 86/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0352 - acc: 0.0081 - val_loss: 0.0301 - val_acc: 0.0116\n",
      "Epoch 87/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0352 - acc: 0.0081 - val_loss: 0.0301 - val_acc: 0.0126\n",
      "Epoch 88/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0352 - acc: 0.0090 - val_loss: 0.0299 - val_acc: 0.0110\n",
      "Epoch 89/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0350 - acc: 0.0092 - val_loss: 0.0298 - val_acc: 0.0116\n",
      "Epoch 90/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0351 - acc: 0.0092 - val_loss: 0.0299 - val_acc: 0.0131\n",
      "Epoch 91/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0350 - acc: 0.0096 - val_loss: 0.0297 - val_acc: 0.0105\n",
      "Epoch 92/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0350 - acc: 0.0097 - val_loss: 0.0297 - val_acc: 0.0131\n",
      "Epoch 93/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0349 - acc: 0.0092 - val_loss: 0.0295 - val_acc: 0.0141\n",
      "Epoch 94/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0350 - acc: 0.0083 - val_loss: 0.0295 - val_acc: 0.0121\n",
      "Epoch 95/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0350 - acc: 0.0092 - val_loss: 0.0295 - val_acc: 0.0116\n",
      "Epoch 96/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0349 - acc: 0.0096 - val_loss: 0.0294 - val_acc: 0.0136\n",
      "Epoch 97/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0348 - acc: 0.0086 - val_loss: 0.0293 - val_acc: 0.0110\n",
      "Epoch 98/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0348 - acc: 0.0093 - val_loss: 0.0292 - val_acc: 0.0116\n",
      "Epoch 99/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0347 - acc: 0.0092 - val_loss: 0.0292 - val_acc: 0.0121\n",
      "Epoch 100/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0346 - acc: 0.0090 - val_loss: 0.0291 - val_acc: 0.0105\n",
      "Epoch 101/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0346 - acc: 0.0096 - val_loss: 0.0291 - val_acc: 0.0116\n",
      "Epoch 102/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0346 - acc: 0.0087 - val_loss: 0.0290 - val_acc: 0.0121\n",
      "Epoch 103/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0346 - acc: 0.0092 - val_loss: 0.0290 - val_acc: 0.0105\n",
      "Epoch 104/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0347 - acc: 0.0088 - val_loss: 0.0289 - val_acc: 0.0121\n",
      "Epoch 105/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0346 - acc: 0.0093 - val_loss: 0.0289 - val_acc: 0.0116\n",
      "Epoch 106/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0345 - acc: 0.0081 - val_loss: 0.0288 - val_acc: 0.0100\n",
      "Epoch 107/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0345 - acc: 0.0100 - val_loss: 0.0287 - val_acc: 0.0090\n",
      "Epoch 108/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0345 - acc: 0.0074 - val_loss: 0.0287 - val_acc: 0.0116\n",
      "Epoch 109/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0344 - acc: 0.0107 - val_loss: 0.0287 - val_acc: 0.0110\n",
      "Epoch 110/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0345 - acc: 0.0086 - val_loss: 0.0287 - val_acc: 0.0126\n",
      "Epoch 111/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0344 - acc: 0.0101 - val_loss: 0.0285 - val_acc: 0.0105\n",
      "Epoch 112/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0343 - acc: 0.0083 - val_loss: 0.0285 - val_acc: 0.0090\n",
      "Epoch 113/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0342 - acc: 0.0107 - val_loss: 0.0284 - val_acc: 0.0105\n",
      "Epoch 114/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0343 - acc: 0.0078 - val_loss: 0.0284 - val_acc: 0.0121\n",
      "Epoch 115/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0344 - acc: 0.0093 - val_loss: 0.0284 - val_acc: 0.0110\n",
      "Epoch 116/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0343 - acc: 0.0097 - val_loss: 0.0283 - val_acc: 0.0105\n",
      "Epoch 117/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0341 - acc: 0.0098 - val_loss: 0.0282 - val_acc: 0.0090\n",
      "Epoch 118/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0342 - acc: 0.0081 - val_loss: 0.0283 - val_acc: 0.0100\n",
      "Epoch 119/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0342 - acc: 0.0087 - val_loss: 0.0282 - val_acc: 0.0085\n",
      "Epoch 120/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0342 - acc: 0.0113 - val_loss: 0.0282 - val_acc: 0.0095\n",
      "Epoch 121/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0102 - val_loss: 0.0281 - val_acc: 0.0105\n",
      "Epoch 122/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0099 - val_loss: 0.0280 - val_acc: 0.0085\n",
      "Epoch 123/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0092 - val_loss: 0.0280 - val_acc: 0.0121\n",
      "Epoch 124/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0341 - acc: 0.0109 - val_loss: 0.0280 - val_acc: 0.0121\n",
      "Epoch 125/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0094 - val_loss: 0.0280 - val_acc: 0.0116\n",
      "Epoch 126/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0098 - val_loss: 0.0279 - val_acc: 0.0110\n",
      "Epoch 127/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 1s - loss: 0.0341 - acc: 0.0101 - val_loss: 0.0279 - val_acc: 0.0100\n",
      "Epoch 128/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0083 - val_loss: 0.0278 - val_acc: 0.0100\n",
      "Epoch 129/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0092 - val_loss: 0.0278 - val_acc: 0.0116\n",
      "Epoch 130/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0339 - acc: 0.0092 - val_loss: 0.0277 - val_acc: 0.0105\n",
      "Epoch 131/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0340 - acc: 0.0084 - val_loss: 0.0278 - val_acc: 0.0110\n",
      "Epoch 132/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0339 - acc: 0.0088 - val_loss: 0.0277 - val_acc: 0.0105\n",
      "Epoch 133/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0339 - acc: 0.0083 - val_loss: 0.0277 - val_acc: 0.0105\n",
      "Epoch 134/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0338 - acc: 0.0080 - val_loss: 0.0276 - val_acc: 0.0105\n",
      "Epoch 135/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0338 - acc: 0.0085 - val_loss: 0.0275 - val_acc: 0.0105\n",
      "Epoch 136/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0338 - acc: 0.0085 - val_loss: 0.0275 - val_acc: 0.0100\n",
      "Epoch 137/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0339 - acc: 0.0099 - val_loss: 0.0275 - val_acc: 0.0100\n",
      "Epoch 138/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0086 - val_loss: 0.0275 - val_acc: 0.0100\n",
      "Epoch 139/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0096 - val_loss: 0.0274 - val_acc: 0.0105\n",
      "Epoch 140/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0087 - val_loss: 0.0274 - val_acc: 0.0116\n",
      "Epoch 141/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0088 - val_loss: 0.0274 - val_acc: 0.0100\n",
      "Epoch 142/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0092 - val_loss: 0.0274 - val_acc: 0.0121\n",
      "Epoch 143/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0094 - val_loss: 0.0273 - val_acc: 0.0095\n",
      "Epoch 144/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0084 - val_loss: 0.0273 - val_acc: 0.0116\n",
      "Epoch 145/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0095 - val_loss: 0.0274 - val_acc: 0.0121\n",
      "Epoch 146/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0105 - val_loss: 0.0273 - val_acc: 0.0110\n",
      "Epoch 147/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0096 - val_loss: 0.0272 - val_acc: 0.0090\n",
      "Epoch 148/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0103 - val_loss: 0.0273 - val_acc: 0.0121\n",
      "Epoch 149/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0337 - acc: 0.0104 - val_loss: 0.0273 - val_acc: 0.0100\n",
      "Epoch 150/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0092 - val_loss: 0.0272 - val_acc: 0.0121\n",
      "Epoch 151/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0335 - acc: 0.0095 - val_loss: 0.0271 - val_acc: 0.0121\n",
      "Epoch 152/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0335 - acc: 0.0099 - val_loss: 0.0271 - val_acc: 0.0121\n",
      "Epoch 153/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0087 - val_loss: 0.0271 - val_acc: 0.0105\n",
      "Epoch 154/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0335 - acc: 0.0081 - val_loss: 0.0271 - val_acc: 0.0126\n",
      "Epoch 155/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0079 - val_loss: 0.0271 - val_acc: 0.0110\n",
      "Epoch 156/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0333 - acc: 0.0098 - val_loss: 0.0270 - val_acc: 0.0110\n",
      "Epoch 157/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0334 - acc: 0.0078 - val_loss: 0.0271 - val_acc: 0.0105\n",
      "Epoch 158/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0334 - acc: 0.0094 - val_loss: 0.0270 - val_acc: 0.0116\n",
      "Epoch 159/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0335 - acc: 0.0099 - val_loss: 0.0269 - val_acc: 0.0105\n",
      "Epoch 160/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0333 - acc: 0.0093 - val_loss: 0.0270 - val_acc: 0.0095\n",
      "Epoch 161/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0336 - acc: 0.0098 - val_loss: 0.0270 - val_acc: 0.0100\n",
      "Epoch 162/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0334 - acc: 0.0079 - val_loss: 0.0270 - val_acc: 0.0100\n",
      "Epoch 163/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0334 - acc: 0.0092 - val_loss: 0.0269 - val_acc: 0.0100\n",
      "Epoch 164/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0333 - acc: 0.0092 - val_loss: 0.0269 - val_acc: 0.0100\n",
      "Epoch 165/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0333 - acc: 0.0092 - val_loss: 0.0269 - val_acc: 0.0105\n",
      "Epoch 166/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0334 - acc: 0.0087 - val_loss: 0.0269 - val_acc: 0.0110\n",
      "Epoch 167/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0333 - acc: 0.0098 - val_loss: 0.0268 - val_acc: 0.0110\n",
      "Epoch 168/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0332 - acc: 0.0085 - val_loss: 0.0267 - val_acc: 0.0095\n",
      "Epoch 169/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0332 - acc: 0.0089 - val_loss: 0.0268 - val_acc: 0.0116\n",
      "Epoch 170/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0332 - acc: 0.0099 - val_loss: 0.0267 - val_acc: 0.0110\n",
      "Epoch 171/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0334 - acc: 0.0098 - val_loss: 0.0267 - val_acc: 0.0131\n",
      "Epoch 172/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0333 - acc: 0.0092 - val_loss: 0.0267 - val_acc: 0.0110\n",
      "Epoch 173/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0332 - acc: 0.0088 - val_loss: 0.0266 - val_acc: 0.0126\n",
      "Epoch 174/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0332 - acc: 0.0112 - val_loss: 0.0267 - val_acc: 0.0100\n",
      "Epoch 175/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0332 - acc: 0.0095 - val_loss: 0.0266 - val_acc: 0.0116\n",
      "Epoch 176/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0330 - acc: 0.0092 - val_loss: 0.0266 - val_acc: 0.0110\n",
      "Epoch 177/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0332 - acc: 0.0096 - val_loss: 0.0267 - val_acc: 0.0105\n",
      "Epoch 178/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0332 - acc: 0.0103 - val_loss: 0.0266 - val_acc: 0.0116\n",
      "Epoch 179/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0331 - acc: 0.0088 - val_loss: 0.0265 - val_acc: 0.0126\n",
      "Epoch 180/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0332 - acc: 0.0095 - val_loss: 0.0265 - val_acc: 0.0100\n",
      "Epoch 181/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0332 - acc: 0.0094 - val_loss: 0.0265 - val_acc: 0.0105\n",
      "Epoch 182/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0331 - acc: 0.0090 - val_loss: 0.0265 - val_acc: 0.0126\n",
      "Epoch 183/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0331 - acc: 0.0095 - val_loss: 0.0265 - val_acc: 0.0105\n",
      "Epoch 184/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0330 - acc: 0.0088 - val_loss: 0.0265 - val_acc: 0.0116\n",
      "Epoch 185/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0331 - acc: 0.0094 - val_loss: 0.0264 - val_acc: 0.0100\n",
      "Epoch 186/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0330 - acc: 0.0098 - val_loss: 0.0264 - val_acc: 0.0085\n",
      "Epoch 187/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0332 - acc: 0.0106 - val_loss: 0.0265 - val_acc: 0.0116\n",
      "Epoch 188/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0330 - acc: 0.0096 - val_loss: 0.0263 - val_acc: 0.0110\n",
      "Epoch 189/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0330 - acc: 0.0085 - val_loss: 0.0263 - val_acc: 0.0100\n",
      "Epoch 190/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0330 - acc: 0.0102 - val_loss: 0.0264 - val_acc: 0.0105\n",
      "Epoch 191/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0331 - acc: 0.0089 - val_loss: 0.0264 - val_acc: 0.0100\n",
      "Epoch 192/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0330 - acc: 0.0098 - val_loss: 0.0264 - val_acc: 0.0110\n",
      "Epoch 193/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0330 - acc: 0.0111 - val_loss: 0.0262 - val_acc: 0.0121\n",
      "Epoch 194/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0329 - acc: 0.0098 - val_loss: 0.0263 - val_acc: 0.0105\n",
      "Epoch 195/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0330 - acc: 0.0098 - val_loss: 0.0263 - val_acc: 0.0121\n",
      "Epoch 196/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0329 - acc: 0.0107 - val_loss: 0.0262 - val_acc: 0.0100\n",
      "Epoch 197/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0330 - acc: 0.0087 - val_loss: 0.0262 - val_acc: 0.0095\n",
      "Epoch 198/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0328 - acc: 0.0099 - val_loss: 0.0262 - val_acc: 0.0095\n",
      "Epoch 199/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0329 - acc: 0.0090 - val_loss: 0.0262 - val_acc: 0.0121\n",
      "Epoch 200/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0328 - acc: 0.0098 - val_loss: 0.0262 - val_acc: 0.0121\n",
      "Epoch 201/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0329 - acc: 0.0098 - val_loss: 0.0262 - val_acc: 0.0116\n",
      "Epoch 202/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0329 - acc: 0.0093 - val_loss: 0.0262 - val_acc: 0.0110\n",
      "Epoch 203/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0329 - acc: 0.0110 - val_loss: 0.0262 - val_acc: 0.0110\n",
      "Epoch 204/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0328 - acc: 0.0098 - val_loss: 0.0261 - val_acc: 0.0105\n",
      "Epoch 205/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0329 - acc: 0.0090 - val_loss: 0.0261 - val_acc: 0.0100\n",
      "Epoch 206/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0329 - acc: 0.0105 - val_loss: 0.0261 - val_acc: 0.0100\n",
      "Epoch 207/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0329 - acc: 0.0098 - val_loss: 0.0261 - val_acc: 0.0121\n",
      "Epoch 208/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0328 - acc: 0.0101 - val_loss: 0.0261 - val_acc: 0.0126\n",
      "Epoch 209/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0110 - val_loss: 0.0261 - val_acc: 0.0110\n",
      "Epoch 210/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0094 - val_loss: 0.0260 - val_acc: 0.0110\n",
      "Epoch 211/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0087 - val_loss: 0.0261 - val_acc: 0.0126\n",
      "Epoch 212/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0101 - val_loss: 0.0261 - val_acc: 0.0100\n",
      "Epoch 213/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0092 - val_loss: 0.0260 - val_acc: 0.0116\n",
      "Epoch 214/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0097 - val_loss: 0.0259 - val_acc: 0.0110\n",
      "Epoch 215/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0106 - val_loss: 0.0260 - val_acc: 0.0105\n",
      "Epoch 216/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0107 - val_loss: 0.0260 - val_acc: 0.0110\n",
      "Epoch 217/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0116 - val_loss: 0.0260 - val_acc: 0.0110\n",
      "Epoch 218/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0326 - acc: 0.0113 - val_loss: 0.0259 - val_acc: 0.0105\n",
      "Epoch 219/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0104 - val_loss: 0.0259 - val_acc: 0.0100\n",
      "Epoch 220/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0108 - val_loss: 0.0260 - val_acc: 0.0090\n",
      "Epoch 221/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0096 - val_loss: 0.0259 - val_acc: 0.0105\n",
      "Epoch 222/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0109 - val_loss: 0.0258 - val_acc: 0.0110\n",
      "Epoch 223/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0328 - acc: 0.0115 - val_loss: 0.0259 - val_acc: 0.0121\n",
      "Epoch 224/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0100 - val_loss: 0.0258 - val_acc: 0.0121\n",
      "Epoch 225/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0094 - val_loss: 0.0259 - val_acc: 0.0105\n",
      "Epoch 226/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0108 - val_loss: 0.0258 - val_acc: 0.0110\n",
      "Epoch 227/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0098 - val_loss: 0.0258 - val_acc: 0.0100\n",
      "Epoch 228/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0095 - val_loss: 0.0259 - val_acc: 0.0121\n",
      "Epoch 229/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0327 - acc: 0.0105 - val_loss: 0.0258 - val_acc: 0.0110\n",
      "Epoch 230/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0103 - val_loss: 0.0259 - val_acc: 0.0105\n",
      "Epoch 231/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0113 - val_loss: 0.0258 - val_acc: 0.0116\n",
      "Epoch 232/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0118 - val_loss: 0.0258 - val_acc: 0.0116\n",
      "Epoch 233/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0109 - val_loss: 0.0257 - val_acc: 0.0110\n",
      "Epoch 234/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0092 - val_loss: 0.0257 - val_acc: 0.0100\n",
      "Epoch 235/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0098 - val_loss: 0.0258 - val_acc: 0.0116\n",
      "Epoch 236/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0107 - val_loss: 0.0257 - val_acc: 0.0126\n",
      "Epoch 237/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0120 - val_loss: 0.0257 - val_acc: 0.0110\n",
      "Epoch 238/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0120 - val_loss: 0.0258 - val_acc: 0.0110\n",
      "Epoch 239/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0089 - val_loss: 0.0257 - val_acc: 0.0095\n",
      "Epoch 240/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0326 - acc: 0.0096 - val_loss: 0.0258 - val_acc: 0.0110\n",
      "Epoch 241/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0114 - val_loss: 0.0258 - val_acc: 0.0121\n",
      "Epoch 242/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0092 - val_loss: 0.0257 - val_acc: 0.0136\n",
      "Epoch 243/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0098 - val_loss: 0.0257 - val_acc: 0.0121\n",
      "Epoch 244/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0103 - val_loss: 0.0257 - val_acc: 0.0105\n",
      "Epoch 245/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0127 - val_loss: 0.0257 - val_acc: 0.0100\n",
      "Epoch 246/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0121 - val_loss: 0.0256 - val_acc: 0.0110\n",
      "Epoch 247/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0099 - val_loss: 0.0258 - val_acc: 0.0105\n",
      "Epoch 248/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0103 - val_loss: 0.0257 - val_acc: 0.0116\n",
      "Epoch 249/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0100 - val_loss: 0.0256 - val_acc: 0.0110\n",
      "Epoch 250/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0105 - val_loss: 0.0257 - val_acc: 0.0100\n",
      "Epoch 251/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0099 - val_loss: 0.0257 - val_acc: 0.0100\n",
      "Epoch 252/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0325 - acc: 0.0120 - val_loss: 0.0256 - val_acc: 0.0136\n",
      "Epoch 253/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11791/11791 [==============================] - 0s - loss: 0.0323 - acc: 0.0103 - val_loss: 0.0256 - val_acc: 0.0116\n",
      "Epoch 254/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0323 - acc: 0.0113 - val_loss: 0.0256 - val_acc: 0.0105\n",
      "Epoch 255/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0105 - val_loss: 0.0256 - val_acc: 0.0116\n",
      "Epoch 256/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0113 - val_loss: 0.0256 - val_acc: 0.0121\n",
      "Epoch 257/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0114 - val_loss: 0.0256 - val_acc: 0.0116\n",
      "Epoch 258/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0324 - acc: 0.0100 - val_loss: 0.0256 - val_acc: 0.0126\n",
      "Epoch 259/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0322 - acc: 0.0126 - val_loss: 0.0256 - val_acc: 0.0121\n",
      "Epoch 260/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0098 - val_loss: 0.0256 - val_acc: 0.0116\n",
      "Epoch 261/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0322 - acc: 0.0130 - val_loss: 0.0256 - val_acc: 0.0110\n",
      "Epoch 262/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0109 - val_loss: 0.0256 - val_acc: 0.0105\n",
      "Epoch 263/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0119 - val_loss: 0.0256 - val_acc: 0.0100\n",
      "Epoch 264/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0110 - val_loss: 0.0255 - val_acc: 0.0121\n",
      "Epoch 265/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0098 - val_loss: 0.0256 - val_acc: 0.0110\n",
      "Epoch 266/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0109 - val_loss: 0.0256 - val_acc: 0.0105\n",
      "Epoch 267/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0115 - val_loss: 0.0255 - val_acc: 0.0110\n",
      "Epoch 268/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0118 - val_loss: 0.0256 - val_acc: 0.0126\n",
      "Epoch 269/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0324 - acc: 0.0127 - val_loss: 0.0257 - val_acc: 0.0121\n",
      "Epoch 270/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0103 - val_loss: 0.0256 - val_acc: 0.0121\n",
      "Epoch 271/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0105 - val_loss: 0.0255 - val_acc: 0.0100\n",
      "Epoch 272/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0114 - val_loss: 0.0255 - val_acc: 0.0126\n",
      "Epoch 273/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0320 - acc: 0.0098 - val_loss: 0.0255 - val_acc: 0.0116\n",
      "Epoch 274/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0323 - acc: 0.0103 - val_loss: 0.0256 - val_acc: 0.0105\n",
      "Epoch 275/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0112 - val_loss: 0.0255 - val_acc: 0.0110\n",
      "Epoch 276/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0321 - acc: 0.0106 - val_loss: 0.0256 - val_acc: 0.0121\n",
      "Epoch 277/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0105 - val_loss: 0.0255 - val_acc: 0.0105\n",
      "Epoch 278/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0093 - val_loss: 0.0255 - val_acc: 0.0121\n",
      "Epoch 279/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0320 - acc: 0.0092 - val_loss: 0.0256 - val_acc: 0.0136\n",
      "Epoch 280/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0321 - acc: 0.0114 - val_loss: 0.0255 - val_acc: 0.0116\n",
      "Epoch 281/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0321 - acc: 0.0120 - val_loss: 0.0255 - val_acc: 0.0116\n",
      "Epoch 282/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0321 - acc: 0.0106 - val_loss: 0.0255 - val_acc: 0.0105\n",
      "Epoch 283/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0111 - val_loss: 0.0256 - val_acc: 0.0131\n",
      "Epoch 284/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0321 - acc: 0.0112 - val_loss: 0.0255 - val_acc: 0.0141\n",
      "Epoch 285/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0320 - acc: 0.0112 - val_loss: 0.0255 - val_acc: 0.0131\n",
      "Epoch 286/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0322 - acc: 0.0103 - val_loss: 0.0255 - val_acc: 0.0126\n",
      "Epoch 287/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0321 - acc: 0.0114 - val_loss: 0.0255 - val_acc: 0.0121\n",
      "Epoch 288/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0320 - acc: 0.0114 - val_loss: 0.0255 - val_acc: 0.0121\n",
      "Epoch 289/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0321 - acc: 0.0116 - val_loss: 0.0255 - val_acc: 0.0121\n",
      "Epoch 290/300\n",
      "11791/11791 [==============================] - 0s - loss: 0.0321 - acc: 0.0106 - val_loss: 0.0255 - val_acc: 0.0126\n",
      "Epoch 291/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0320 - acc: 0.0103 - val_loss: 0.0254 - val_acc: 0.0121\n",
      "Epoch 292/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0320 - acc: 0.0118 - val_loss: 0.0254 - val_acc: 0.0110\n",
      "Epoch 293/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0320 - acc: 0.0110 - val_loss: 0.0254 - val_acc: 0.0121\n",
      "Epoch 294/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0319 - acc: 0.0109 - val_loss: 0.0254 - val_acc: 0.0126\n",
      "Epoch 295/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0321 - acc: 0.0118 - val_loss: 0.0254 - val_acc: 0.0110\n",
      "Epoch 296/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0321 - acc: 0.0103 - val_loss: 0.0255 - val_acc: 0.0151\n",
      "Epoch 297/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0320 - acc: 0.0118 - val_loss: 0.0256 - val_acc: 0.0126\n",
      "Epoch 298/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0320 - acc: 0.0137 - val_loss: 0.0254 - val_acc: 0.0136\n",
      "Epoch 299/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0320 - acc: 0.0119 - val_loss: 0.0254 - val_acc: 0.0105\n",
      "Epoch 300/300\n",
      "11791/11791 [==============================] - 1s - loss: 0.0319 - acc: 0.0120 - val_loss: 0.0254 - val_acc: 0.0131\n",
      "Test loss: 0.0253762739291\n",
      "Test accuracy: 0.01305876444\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_vec, x_train_vec,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_vec, x_test_vec))\n",
    "score = model.evaluate(x_test_vec, x_test_vec, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エポック数は先ほどより長くしたこともあり少々待ち時間がかかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21b88633ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    predicted = model.predict(x_train_vec[i:i+1]).reshape((28,28))\n",
    "    plt.imshow(predicted)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一部はっきりしないweak ４もあるが、単層と比べてかなり良くなった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
