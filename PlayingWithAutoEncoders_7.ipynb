{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを０～９のすべてに拡大してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 300\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_train_idx = np.logical_or(y_train > -1, False)\n",
    "keep_test_idx = np.logical_or(y_test > -1, False)\n",
    "\n",
    "x_train = x_train[keep_train_idx]\n",
    "x_test = x_test[keep_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_vec = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test_vec = x_test.reshape(x_test.shape[0], 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中央の要素数は5個に設定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 505       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               600       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 784)               79184     \n",
      "=================================================================\n",
      "Total params: 158,789\n",
      "Trainable params: 158,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(784, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0708 - acc: 0.0095 - val_loss: 0.0634 - val_acc: 0.0124\n",
      "Epoch 2/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0628 - acc: 0.0105 - val_loss: 0.0613 - val_acc: 0.0107\n",
      "Epoch 3/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0620 - acc: 0.0104 - val_loss: 0.0602 - val_acc: 0.0137\n",
      "Epoch 4/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0602 - acc: 0.0131 - val_loss: 0.0567 - val_acc: 0.0096\n",
      "Epoch 5/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0591 - acc: 0.0123 - val_loss: 0.0561 - val_acc: 0.0116\n",
      "Epoch 6/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0587 - acc: 0.0130 - val_loss: 0.0556 - val_acc: 0.0155\n",
      "Epoch 7/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0584 - acc: 0.0129 - val_loss: 0.0551 - val_acc: 0.0160\n",
      "Epoch 8/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0581 - acc: 0.0146 - val_loss: 0.0546 - val_acc: 0.0158\n",
      "Epoch 9/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0578 - acc: 0.0144 - val_loss: 0.0541 - val_acc: 0.0127\n",
      "Epoch 10/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0574 - acc: 0.0148 - val_loss: 0.0533 - val_acc: 0.0147\n",
      "Epoch 11/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0568 - acc: 0.0147 - val_loss: 0.0521 - val_acc: 0.0164\n",
      "Epoch 12/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0562 - acc: 0.0121 - val_loss: 0.0513 - val_acc: 0.0124\n",
      "Epoch 13/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0558 - acc: 0.0118 - val_loss: 0.0508 - val_acc: 0.0154\n",
      "Epoch 14/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0555 - acc: 0.0103 - val_loss: 0.0502 - val_acc: 0.0144\n",
      "Epoch 15/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0552 - acc: 0.0109 - val_loss: 0.0497 - val_acc: 0.0125\n",
      "Epoch 16/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0550 - acc: 0.0106 - val_loss: 0.0495 - val_acc: 0.0133\n",
      "Epoch 17/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0547 - acc: 0.0102 - val_loss: 0.0492 - val_acc: 0.0117\n",
      "Epoch 18/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0545 - acc: 0.0101 - val_loss: 0.0490 - val_acc: 0.0100\n",
      "Epoch 19/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0544 - acc: 0.0090 - val_loss: 0.0488 - val_acc: 0.0126\n",
      "Epoch 20/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0542 - acc: 0.0099 - val_loss: 0.0487 - val_acc: 0.0097\n",
      "Epoch 21/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0541 - acc: 0.0096 - val_loss: 0.0487 - val_acc: 0.0111\n",
      "Epoch 22/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0540 - acc: 0.0106 - val_loss: 0.0484 - val_acc: 0.0103\n",
      "Epoch 23/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0538 - acc: 0.0100 - val_loss: 0.0483 - val_acc: 0.0136\n",
      "Epoch 24/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0537 - acc: 0.0095 - val_loss: 0.0482 - val_acc: 0.0095\n",
      "Epoch 25/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0536 - acc: 0.0103 - val_loss: 0.0481 - val_acc: 0.0126\n",
      "Epoch 26/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0535 - acc: 0.0097 - val_loss: 0.0480 - val_acc: 0.0113\n",
      "Epoch 27/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0534 - acc: 0.0104 - val_loss: 0.0478 - val_acc: 0.0137\n",
      "Epoch 28/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0533 - acc: 0.0106 - val_loss: 0.0478 - val_acc: 0.0118\n",
      "Epoch 29/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0532 - acc: 0.0089 - val_loss: 0.0477 - val_acc: 0.0133\n",
      "Epoch 30/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0532 - acc: 0.0095 - val_loss: 0.0477 - val_acc: 0.0115\n",
      "Epoch 31/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0531 - acc: 0.0089 - val_loss: 0.0476 - val_acc: 0.0108\n",
      "Epoch 32/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0531 - acc: 0.0095 - val_loss: 0.0478 - val_acc: 0.0107\n",
      "Epoch 33/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0529 - acc: 0.0094 - val_loss: 0.0474 - val_acc: 0.0116\n",
      "Epoch 34/300\n",
      "60000/60000 [==============================] - 7s - loss: 0.0529 - acc: 0.0091 - val_loss: 0.0474 - val_acc: 0.0115\n",
      "Epoch 35/300\n",
      "60000/60000 [==============================] - 7s - loss: 0.0528 - acc: 0.0095 - val_loss: 0.0473 - val_acc: 0.0126\n",
      "Epoch 36/300\n",
      "60000/60000 [==============================] - 7s - loss: 0.0528 - acc: 0.0099 - val_loss: 0.0474 - val_acc: 0.0113\n",
      "Epoch 37/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0528 - acc: 0.0105 - val_loss: 0.0472 - val_acc: 0.0115\n",
      "Epoch 38/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0527 - acc: 0.0091 - val_loss: 0.0472 - val_acc: 0.0116\n",
      "Epoch 39/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0527 - acc: 0.0100 - val_loss: 0.0472 - val_acc: 0.0130\n",
      "Epoch 40/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0526 - acc: 0.0097 - val_loss: 0.0469 - val_acc: 0.0108\n",
      "Epoch 41/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0525 - acc: 0.0102 - val_loss: 0.0471 - val_acc: 0.0110\n",
      "Epoch 42/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0525 - acc: 0.0097 - val_loss: 0.0472 - val_acc: 0.0117\n",
      "Epoch 43/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0524 - acc: 0.0094 - val_loss: 0.0470 - val_acc: 0.0115\n",
      "Epoch 44/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0524 - acc: 0.0096 - val_loss: 0.0469 - val_acc: 0.0120\n",
      "Epoch 45/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0524 - acc: 0.0097 - val_loss: 0.0469 - val_acc: 0.0127\n",
      "Epoch 46/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0522 - acc: 0.0098 - val_loss: 0.0468 - val_acc: 0.0104\n",
      "Epoch 47/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0522 - acc: 0.0098 - val_loss: 0.0467 - val_acc: 0.0134\n",
      "Epoch 48/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0522 - acc: 0.0093 - val_loss: 0.0468 - val_acc: 0.0114\n",
      "Epoch 49/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0521 - acc: 0.0095 - val_loss: 0.0466 - val_acc: 0.0120\n",
      "Epoch 50/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0521 - acc: 0.0098 - val_loss: 0.0466 - val_acc: 0.0127\n",
      "Epoch 51/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0521 - acc: 0.0102 - val_loss: 0.0466 - val_acc: 0.0115\n",
      "Epoch 52/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0520 - acc: 0.0092 - val_loss: 0.0466 - val_acc: 0.0115\n",
      "Epoch 53/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0520 - acc: 0.0090 - val_loss: 0.0466 - val_acc: 0.0108\n",
      "Epoch 54/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0519 - acc: 0.0093 - val_loss: 0.0466 - val_acc: 0.0106\n",
      "Epoch 55/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0519 - acc: 0.0089 - val_loss: 0.0465 - val_acc: 0.0105\n",
      "Epoch 56/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0519 - acc: 0.0099 - val_loss: 0.0465 - val_acc: 0.0101\n",
      "Epoch 57/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0518 - acc: 0.0093 - val_loss: 0.0463 - val_acc: 0.0122\n",
      "Epoch 58/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0518 - acc: 0.0090 - val_loss: 0.0463 - val_acc: 0.0117\n",
      "Epoch 59/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0518 - acc: 0.0088 - val_loss: 0.0465 - val_acc: 0.0107\n",
      "Epoch 60/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0517 - acc: 0.0095 - val_loss: 0.0463 - val_acc: 0.0110\n",
      "Epoch 61/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0516 - acc: 0.0092 - val_loss: 0.0464 - val_acc: 0.0139\n",
      "Epoch 62/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0516 - acc: 0.0096 - val_loss: 0.0465 - val_acc: 0.0109\n",
      "Epoch 63/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0516 - acc: 0.0093 - val_loss: 0.0463 - val_acc: 0.0104\n",
      "Epoch 64/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 5s - loss: 0.0515 - acc: 0.0095 - val_loss: 0.0463 - val_acc: 0.0088\n",
      "Epoch 65/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0516 - acc: 0.0093 - val_loss: 0.0464 - val_acc: 0.0118\n",
      "Epoch 66/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0515 - acc: 0.0089 - val_loss: 0.0463 - val_acc: 0.0110\n",
      "Epoch 67/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0515 - acc: 0.0094 - val_loss: 0.0463 - val_acc: 0.0113\n",
      "Epoch 68/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0514 - acc: 0.0097 - val_loss: 0.0463 - val_acc: 0.0108\n",
      "Epoch 69/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0514 - acc: 0.0099 - val_loss: 0.0461 - val_acc: 0.0091\n",
      "Epoch 70/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0514 - acc: 0.0086 - val_loss: 0.0462 - val_acc: 0.0084\n",
      "Epoch 71/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0514 - acc: 0.0090 - val_loss: 0.0462 - val_acc: 0.0115\n",
      "Epoch 72/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0513 - acc: 0.0092 - val_loss: 0.0464 - val_acc: 0.0116\n",
      "Epoch 73/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0513 - acc: 0.0089 - val_loss: 0.0463 - val_acc: 0.0093\n",
      "Epoch 74/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0513 - acc: 0.0093 - val_loss: 0.0464 - val_acc: 0.0089\n",
      "Epoch 75/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0512 - acc: 0.0099 - val_loss: 0.0464 - val_acc: 0.0112\n",
      "Epoch 76/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0513 - acc: 0.0092 - val_loss: 0.0464 - val_acc: 0.0104\n",
      "Epoch 77/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0512 - acc: 0.0087 - val_loss: 0.0462 - val_acc: 0.0112\n",
      "Epoch 78/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0512 - acc: 0.0099 - val_loss: 0.0465 - val_acc: 0.0101\n",
      "Epoch 79/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0512 - acc: 0.0093 - val_loss: 0.0464 - val_acc: 0.0130\n",
      "Epoch 80/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0512 - acc: 0.0092 - val_loss: 0.0464 - val_acc: 0.0093\n",
      "Epoch 81/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0511 - acc: 0.0093 - val_loss: 0.0464 - val_acc: 0.0093\n",
      "Epoch 82/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0511 - acc: 0.0093 - val_loss: 0.0464 - val_acc: 0.0113\n",
      "Epoch 83/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0510 - acc: 0.0098 - val_loss: 0.0463 - val_acc: 0.0129\n",
      "Epoch 84/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0510 - acc: 0.0093 - val_loss: 0.0463 - val_acc: 0.0090\n",
      "Epoch 85/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0510 - acc: 0.0092 - val_loss: 0.0463 - val_acc: 0.0092\n",
      "Epoch 86/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0510 - acc: 0.0093 - val_loss: 0.0465 - val_acc: 0.0098\n",
      "Epoch 87/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0510 - acc: 0.0094 - val_loss: 0.0463 - val_acc: 0.0104\n",
      "Epoch 88/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0509 - acc: 0.0092 - val_loss: 0.0463 - val_acc: 0.0095\n",
      "Epoch 89/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0509 - acc: 0.0090 - val_loss: 0.0464 - val_acc: 0.0096\n",
      "Epoch 90/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0509 - acc: 0.0089 - val_loss: 0.0465 - val_acc: 0.0091\n",
      "Epoch 91/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0509 - acc: 0.0089 - val_loss: 0.0464 - val_acc: 0.0082\n",
      "Epoch 92/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0509 - acc: 0.0095 - val_loss: 0.0465 - val_acc: 0.0106\n",
      "Epoch 93/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0509 - acc: 0.0090 - val_loss: 0.0465 - val_acc: 0.0089\n",
      "Epoch 94/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0509 - acc: 0.0102 - val_loss: 0.0465 - val_acc: 0.0089\n",
      "Epoch 95/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0508 - acc: 0.0089 - val_loss: 0.0464 - val_acc: 0.0096\n",
      "Epoch 96/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0508 - acc: 0.0094 - val_loss: 0.0463 - val_acc: 0.0092\n",
      "Epoch 97/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0508 - acc: 0.0091 - val_loss: 0.0465 - val_acc: 0.0093\n",
      "Epoch 98/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0508 - acc: 0.0092 - val_loss: 0.0465 - val_acc: 0.0109\n",
      "Epoch 99/300\n",
      "60000/60000 [==============================] - 7s - loss: 0.0508 - acc: 0.0100 - val_loss: 0.0463 - val_acc: 0.0095\n",
      "Epoch 100/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0507 - acc: 0.0100 - val_loss: 0.0464 - val_acc: 0.0093\n",
      "Epoch 101/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0507 - acc: 0.0097 - val_loss: 0.0463 - val_acc: 0.0105\n",
      "Epoch 102/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0507 - acc: 0.0098 - val_loss: 0.0464 - val_acc: 0.0099\n",
      "Epoch 103/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0507 - acc: 0.0088 - val_loss: 0.0464 - val_acc: 0.0083\n",
      "Epoch 104/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0507 - acc: 0.0103 - val_loss: 0.0463 - val_acc: 0.0089\n",
      "Epoch 105/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0506 - acc: 0.0097 - val_loss: 0.0463 - val_acc: 0.0080\n",
      "Epoch 106/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0507 - acc: 0.0094 - val_loss: 0.0465 - val_acc: 0.0091\n",
      "Epoch 107/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0505 - acc: 0.0094 - val_loss: 0.0465 - val_acc: 0.0082\n",
      "Epoch 108/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0506 - acc: 0.0096 - val_loss: 0.0465 - val_acc: 0.0112\n",
      "Epoch 109/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0506 - acc: 0.0097 - val_loss: 0.0466 - val_acc: 0.0106\n",
      "Epoch 110/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0505 - acc: 0.0089 - val_loss: 0.0464 - val_acc: 0.0091\n",
      "Epoch 111/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0505 - acc: 0.0091 - val_loss: 0.0465 - val_acc: 0.0096\n",
      "Epoch 112/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0505 - acc: 0.0105 - val_loss: 0.0465 - val_acc: 0.0106\n",
      "Epoch 113/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0505 - acc: 0.0095 - val_loss: 0.0467 - val_acc: 0.0108\n",
      "Epoch 114/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0505 - acc: 0.0096 - val_loss: 0.0465 - val_acc: 0.0104\n",
      "Epoch 115/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0505 - acc: 0.0096 - val_loss: 0.0464 - val_acc: 0.0098\n",
      "Epoch 116/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0505 - acc: 0.0096 - val_loss: 0.0465 - val_acc: 0.0088\n",
      "Epoch 117/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0504 - acc: 0.0095 - val_loss: 0.0466 - val_acc: 0.0089\n",
      "Epoch 118/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0505 - acc: 0.0088 - val_loss: 0.0465 - val_acc: 0.0100\n",
      "Epoch 119/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0505 - acc: 0.0101 - val_loss: 0.0466 - val_acc: 0.0095\n",
      "Epoch 120/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0505 - acc: 0.0094 - val_loss: 0.0465 - val_acc: 0.0107\n",
      "Epoch 121/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0505 - acc: 0.0096 - val_loss: 0.0465 - val_acc: 0.0096\n",
      "Epoch 122/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0504 - acc: 0.0097 - val_loss: 0.0466 - val_acc: 0.0079\n",
      "Epoch 123/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0504 - acc: 0.0099 - val_loss: 0.0464 - val_acc: 0.0093\n",
      "Epoch 124/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0504 - acc: 0.0098 - val_loss: 0.0467 - val_acc: 0.0084\n",
      "Epoch 125/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0504 - acc: 0.0094 - val_loss: 0.0467 - val_acc: 0.0099\n",
      "Epoch 126/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0504 - acc: 0.0099 - val_loss: 0.0466 - val_acc: 0.0076\n",
      "Epoch 127/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 5s - loss: 0.0503 - acc: 0.0102 - val_loss: 0.0466 - val_acc: 0.0091\n",
      "Epoch 128/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0504 - acc: 0.0091 - val_loss: 0.0467 - val_acc: 0.0083\n",
      "Epoch 129/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0504 - acc: 0.0093 - val_loss: 0.0469 - val_acc: 0.0086\n",
      "Epoch 130/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0503 - acc: 0.0096 - val_loss: 0.0467 - val_acc: 0.0082\n",
      "Epoch 131/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0503 - acc: 0.0097 - val_loss: 0.0467 - val_acc: 0.0080\n",
      "Epoch 132/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0503 - acc: 0.0090 - val_loss: 0.0468 - val_acc: 0.0080\n",
      "Epoch 133/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0503 - acc: 0.0098 - val_loss: 0.0469 - val_acc: 0.0076\n",
      "Epoch 134/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0502 - acc: 0.0090 - val_loss: 0.0470 - val_acc: 0.0082\n",
      "Epoch 135/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0502 - acc: 0.0095 - val_loss: 0.0466 - val_acc: 0.0083\n",
      "Epoch 136/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0503 - acc: 0.0099 - val_loss: 0.0467 - val_acc: 0.0074\n",
      "Epoch 137/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0503 - acc: 0.0095 - val_loss: 0.0466 - val_acc: 0.0090\n",
      "Epoch 138/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0502 - acc: 0.0094 - val_loss: 0.0468 - val_acc: 0.0088\n",
      "Epoch 139/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0502 - acc: 0.0098 - val_loss: 0.0468 - val_acc: 0.0094\n",
      "Epoch 140/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0502 - acc: 0.0094 - val_loss: 0.0467 - val_acc: 0.0099\n",
      "Epoch 141/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0503 - acc: 0.0103 - val_loss: 0.0470 - val_acc: 0.0072\n",
      "Epoch 142/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0503 - acc: 0.0101 - val_loss: 0.0467 - val_acc: 0.0085\n",
      "Epoch 143/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0502 - acc: 0.0100 - val_loss: 0.0468 - val_acc: 0.0102\n",
      "Epoch 144/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0502 - acc: 0.0093 - val_loss: 0.0469 - val_acc: 0.0104\n",
      "Epoch 145/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0502 - acc: 0.0100 - val_loss: 0.0470 - val_acc: 0.0099\n",
      "Epoch 146/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0501 - acc: 0.0098 - val_loss: 0.0468 - val_acc: 0.0075\n",
      "Epoch 147/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0502 - acc: 0.0092 - val_loss: 0.0466 - val_acc: 0.0081\n",
      "Epoch 148/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0501 - acc: 0.0099 - val_loss: 0.0469 - val_acc: 0.0075\n",
      "Epoch 149/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0502 - acc: 0.0087 - val_loss: 0.0467 - val_acc: 0.0087\n",
      "Epoch 150/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0501 - acc: 0.0089 - val_loss: 0.0469 - val_acc: 0.0085\n",
      "Epoch 151/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0501 - acc: 0.0092 - val_loss: 0.0471 - val_acc: 0.0084\n",
      "Epoch 152/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0501 - acc: 0.0095 - val_loss: 0.0468 - val_acc: 0.0099\n",
      "Epoch 153/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0501 - acc: 0.0100 - val_loss: 0.0468 - val_acc: 0.0069\n",
      "Epoch 154/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0501 - acc: 0.0095 - val_loss: 0.0470 - val_acc: 0.0075\n",
      "Epoch 155/300\n",
      "60000/60000 [==============================] - 7s - loss: 0.0500 - acc: 0.0091 - val_loss: 0.0470 - val_acc: 0.0086\n",
      "Epoch 156/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0501 - acc: 0.0090 - val_loss: 0.0470 - val_acc: 0.0079\n",
      "Epoch 157/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0501 - acc: 0.0092 - val_loss: 0.0470 - val_acc: 0.0080\n",
      "Epoch 158/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0501 - acc: 0.0093 - val_loss: 0.0468 - val_acc: 0.0074\n",
      "Epoch 159/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0501 - acc: 0.0092 - val_loss: 0.0470 - val_acc: 0.0084\n",
      "Epoch 160/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0093 - val_loss: 0.0473 - val_acc: 0.0077\n",
      "Epoch 161/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0090 - val_loss: 0.0470 - val_acc: 0.0089\n",
      "Epoch 162/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0095 - val_loss: 0.0471 - val_acc: 0.0073\n",
      "Epoch 163/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0098 - val_loss: 0.0470 - val_acc: 0.0096\n",
      "Epoch 164/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0096 - val_loss: 0.0470 - val_acc: 0.0091\n",
      "Epoch 165/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0092 - val_loss: 0.0470 - val_acc: 0.0082\n",
      "Epoch 166/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0091 - val_loss: 0.0470 - val_acc: 0.0080\n",
      "Epoch 167/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0093 - val_loss: 0.0472 - val_acc: 0.0088\n",
      "Epoch 168/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0499 - acc: 0.0100 - val_loss: 0.0469 - val_acc: 0.0089\n",
      "Epoch 169/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0499 - acc: 0.0098 - val_loss: 0.0472 - val_acc: 0.0086\n",
      "Epoch 170/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0500 - acc: 0.0096 - val_loss: 0.0472 - val_acc: 0.0078\n",
      "Epoch 171/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0096 - val_loss: 0.0472 - val_acc: 0.0074\n",
      "Epoch 172/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0499 - acc: 0.0100 - val_loss: 0.0472 - val_acc: 0.0090\n",
      "Epoch 173/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0097 - val_loss: 0.0472 - val_acc: 0.0080\n",
      "Epoch 174/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0500 - acc: 0.0104 - val_loss: 0.0472 - val_acc: 0.0083\n",
      "Epoch 175/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0088 - val_loss: 0.0472 - val_acc: 0.0075\n",
      "Epoch 176/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0499 - acc: 0.0095 - val_loss: 0.0470 - val_acc: 0.0077\n",
      "Epoch 177/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0499 - acc: 0.0091 - val_loss: 0.0474 - val_acc: 0.0085\n",
      "Epoch 178/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0099 - val_loss: 0.0473 - val_acc: 0.0073\n",
      "Epoch 179/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0499 - acc: 0.0092 - val_loss: 0.0471 - val_acc: 0.0084\n",
      "Epoch 180/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0499 - acc: 0.0103 - val_loss: 0.0475 - val_acc: 0.0086\n",
      "Epoch 181/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0499 - acc: 0.0100 - val_loss: 0.0473 - val_acc: 0.0091\n",
      "Epoch 182/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0499 - acc: 0.0100 - val_loss: 0.0472 - val_acc: 0.0073\n",
      "Epoch 183/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0499 - acc: 0.0089 - val_loss: 0.0474 - val_acc: 0.0077\n",
      "Epoch 184/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0092 - val_loss: 0.0473 - val_acc: 0.0082\n",
      "Epoch 185/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0090 - val_loss: 0.0473 - val_acc: 0.0073\n",
      "Epoch 186/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0099 - val_loss: 0.0473 - val_acc: 0.0087\n",
      "Epoch 187/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0097 - val_loss: 0.0472 - val_acc: 0.0075\n",
      "Epoch 188/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0092 - val_loss: 0.0474 - val_acc: 0.0071\n",
      "Epoch 189/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0095 - val_loss: 0.0474 - val_acc: 0.0076\n",
      "Epoch 190/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0088 - val_loss: 0.0476 - val_acc: 0.0067\n",
      "Epoch 191/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0092 - val_loss: 0.0475 - val_acc: 0.0075\n",
      "Epoch 192/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0092 - val_loss: 0.0474 - val_acc: 0.0081\n",
      "Epoch 193/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0089 - val_loss: 0.0473 - val_acc: 0.0077\n",
      "Epoch 194/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0088 - val_loss: 0.0476 - val_acc: 0.0077\n",
      "Epoch 195/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0094 - val_loss: 0.0476 - val_acc: 0.0074\n",
      "Epoch 196/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0091 - val_loss: 0.0475 - val_acc: 0.0081\n",
      "Epoch 197/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0089 - val_loss: 0.0475 - val_acc: 0.0086\n",
      "Epoch 198/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0096 - val_loss: 0.0475 - val_acc: 0.0083\n",
      "Epoch 199/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0089 - val_loss: 0.0475 - val_acc: 0.0073\n",
      "Epoch 200/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0092 - val_loss: 0.0474 - val_acc: 0.0083\n",
      "Epoch 201/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0498 - acc: 0.0092 - val_loss: 0.0476 - val_acc: 0.0090\n",
      "Epoch 202/300\n",
      "60000/60000 [==============================] - 4s - loss: 0.0496 - acc: 0.0087 - val_loss: 0.0473 - val_acc: 0.0081\n",
      "Epoch 203/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0095 - val_loss: 0.0475 - val_acc: 0.0079\n",
      "Epoch 204/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0089 - val_loss: 0.0472 - val_acc: 0.0086\n",
      "Epoch 205/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0085 - val_loss: 0.0475 - val_acc: 0.0076\n",
      "Epoch 206/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0085 - val_loss: 0.0475 - val_acc: 0.0079\n",
      "Epoch 207/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0085 - val_loss: 0.0477 - val_acc: 0.0077\n",
      "Epoch 208/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0094 - val_loss: 0.0474 - val_acc: 0.0076\n",
      "Epoch 209/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0090 - val_loss: 0.0476 - val_acc: 0.0080\n",
      "Epoch 210/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0085 - val_loss: 0.0476 - val_acc: 0.0077\n",
      "Epoch 211/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0088 - val_loss: 0.0476 - val_acc: 0.0080\n",
      "Epoch 212/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0497 - acc: 0.0086 - val_loss: 0.0478 - val_acc: 0.0086\n",
      "Epoch 213/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0090 - val_loss: 0.0475 - val_acc: 0.0087\n",
      "Epoch 214/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0085 - val_loss: 0.0476 - val_acc: 0.0074\n",
      "Epoch 215/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0088 - val_loss: 0.0477 - val_acc: 0.0081\n",
      "Epoch 216/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0085 - val_loss: 0.0476 - val_acc: 0.0075\n",
      "Epoch 217/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0496 - acc: 0.0088 - val_loss: 0.0476 - val_acc: 0.0078\n",
      "Epoch 218/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0495 - acc: 0.0086 - val_loss: 0.0476 - val_acc: 0.0078\n",
      "Epoch 219/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0496 - acc: 0.0085 - val_loss: 0.0475 - val_acc: 0.0083\n",
      "Epoch 220/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0085 - val_loss: 0.0478 - val_acc: 0.0075\n",
      "Epoch 221/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0093 - val_loss: 0.0479 - val_acc: 0.0075\n",
      "Epoch 222/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0089 - val_loss: 0.0476 - val_acc: 0.0073\n",
      "Epoch 223/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0094 - val_loss: 0.0479 - val_acc: 0.0075\n",
      "Epoch 224/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0099 - val_loss: 0.0477 - val_acc: 0.0075\n",
      "Epoch 225/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0091 - val_loss: 0.0477 - val_acc: 0.0082\n",
      "Epoch 226/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0083 - val_loss: 0.0479 - val_acc: 0.0075\n",
      "Epoch 227/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0088 - val_loss: 0.0479 - val_acc: 0.0077\n",
      "Epoch 228/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0086 - val_loss: 0.0478 - val_acc: 0.0078\n",
      "Epoch 229/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0087 - val_loss: 0.0478 - val_acc: 0.0077\n",
      "Epoch 230/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0088 - val_loss: 0.0479 - val_acc: 0.0077\n",
      "Epoch 231/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0091 - val_loss: 0.0480 - val_acc: 0.0074\n",
      "Epoch 232/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0087 - val_loss: 0.0479 - val_acc: 0.0077\n",
      "Epoch 233/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0092 - val_loss: 0.0478 - val_acc: 0.0079\n",
      "Epoch 234/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0087 - val_loss: 0.0476 - val_acc: 0.0077\n",
      "Epoch 235/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0090 - val_loss: 0.0475 - val_acc: 0.0076\n",
      "Epoch 236/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0091 - val_loss: 0.0479 - val_acc: 0.0073\n",
      "Epoch 237/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0089 - val_loss: 0.0478 - val_acc: 0.0073\n",
      "Epoch 238/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0089 - val_loss: 0.0479 - val_acc: 0.0076\n",
      "Epoch 239/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0085 - val_loss: 0.0479 - val_acc: 0.0071\n",
      "Epoch 240/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0496 - acc: 0.0092 - val_loss: 0.0478 - val_acc: 0.0084\n",
      "Epoch 241/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0084 - val_loss: 0.0480 - val_acc: 0.0085\n",
      "Epoch 242/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0086 - val_loss: 0.0479 - val_acc: 0.0091\n",
      "Epoch 243/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0090 - val_loss: 0.0480 - val_acc: 0.0073\n",
      "Epoch 244/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0082 - val_loss: 0.0478 - val_acc: 0.0087\n",
      "Epoch 245/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0089 - val_loss: 0.0477 - val_acc: 0.0078\n",
      "Epoch 246/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0095 - val_loss: 0.0479 - val_acc: 0.0077\n",
      "Epoch 247/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0088 - val_loss: 0.0479 - val_acc: 0.0081\n",
      "Epoch 248/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0495 - acc: 0.0081 - val_loss: 0.0480 - val_acc: 0.0073\n",
      "Epoch 249/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0089 - val_loss: 0.0481 - val_acc: 0.0072\n",
      "Epoch 250/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0495 - acc: 0.0093 - val_loss: 0.0482 - val_acc: 0.0079\n",
      "Epoch 251/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0494 - acc: 0.0088 - val_loss: 0.0479 - val_acc: 0.0072\n",
      "Epoch 252/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0494 - acc: 0.0090 - val_loss: 0.0482 - val_acc: 0.0071\n",
      "Epoch 253/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0084 - val_loss: 0.0479 - val_acc: 0.0090\n",
      "Epoch 254/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0087 - val_loss: 0.0482 - val_acc: 0.0073\n",
      "Epoch 255/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0086 - val_loss: 0.0480 - val_acc: 0.0069\n",
      "Epoch 256/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0091 - val_loss: 0.0484 - val_acc: 0.0070\n",
      "Epoch 257/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0089 - val_loss: 0.0478 - val_acc: 0.0078\n",
      "Epoch 258/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0495 - acc: 0.0082 - val_loss: 0.0483 - val_acc: 0.0081\n",
      "Epoch 259/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0092 - val_loss: 0.0482 - val_acc: 0.0076\n",
      "Epoch 260/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0495 - acc: 0.0082 - val_loss: 0.0477 - val_acc: 0.0072\n",
      "Epoch 261/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0494 - acc: 0.0086 - val_loss: 0.0480 - val_acc: 0.0067\n",
      "Epoch 262/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0086 - val_loss: 0.0481 - val_acc: 0.0074\n",
      "Epoch 263/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0078 - val_loss: 0.0482 - val_acc: 0.0078\n",
      "Epoch 264/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0082 - val_loss: 0.0480 - val_acc: 0.0086\n",
      "Epoch 265/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0493 - acc: 0.0080 - val_loss: 0.0480 - val_acc: 0.0083\n",
      "Epoch 266/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0084 - val_loss: 0.0479 - val_acc: 0.0070\n",
      "Epoch 267/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0085 - val_loss: 0.0483 - val_acc: 0.0075\n",
      "Epoch 268/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0089 - val_loss: 0.0481 - val_acc: 0.0084\n",
      "Epoch 269/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0493 - acc: 0.0086 - val_loss: 0.0483 - val_acc: 0.0082\n",
      "Epoch 270/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0494 - acc: 0.0085 - val_loss: 0.0480 - val_acc: 0.0065\n",
      "Epoch 271/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0492 - acc: 0.0092 - val_loss: 0.0485 - val_acc: 0.0077\n",
      "Epoch 272/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0082 - val_loss: 0.0483 - val_acc: 0.0083\n",
      "Epoch 273/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0090 - val_loss: 0.0482 - val_acc: 0.0079\n",
      "Epoch 274/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0087 - val_loss: 0.0482 - val_acc: 0.0075\n",
      "Epoch 275/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0082 - val_loss: 0.0481 - val_acc: 0.0069\n",
      "Epoch 276/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0087 - val_loss: 0.0482 - val_acc: 0.0081\n",
      "Epoch 277/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0088 - val_loss: 0.0480 - val_acc: 0.0076\n",
      "Epoch 278/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0091 - val_loss: 0.0481 - val_acc: 0.0084\n",
      "Epoch 279/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0088 - val_loss: 0.0484 - val_acc: 0.0071\n",
      "Epoch 280/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0089 - val_loss: 0.0480 - val_acc: 0.0080\n",
      "Epoch 281/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0494 - acc: 0.0075 - val_loss: 0.0480 - val_acc: 0.0077\n",
      "Epoch 282/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0095 - val_loss: 0.0484 - val_acc: 0.0077\n",
      "Epoch 283/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0493 - acc: 0.0086 - val_loss: 0.0482 - val_acc: 0.0073\n",
      "Epoch 284/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0493 - acc: 0.0092 - val_loss: 0.0482 - val_acc: 0.0063\n",
      "Epoch 285/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0494 - acc: 0.0090 - val_loss: 0.0482 - val_acc: 0.0077\n",
      "Epoch 286/300\n",
      "60000/60000 [==============================] - 7s - loss: 0.0493 - acc: 0.0081 - val_loss: 0.0481 - val_acc: 0.0082\n",
      "Epoch 287/300\n",
      "60000/60000 [==============================] - 7s - loss: 0.0493 - acc: 0.0089 - val_loss: 0.0484 - val_acc: 0.0082\n",
      "Epoch 288/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0492 - acc: 0.0096 - val_loss: 0.0485 - val_acc: 0.0080\n",
      "Epoch 289/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0492 - acc: 0.0092 - val_loss: 0.0482 - val_acc: 0.0079\n",
      "Epoch 290/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0493 - acc: 0.0087 - val_loss: 0.0483 - val_acc: 0.0081\n",
      "Epoch 291/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0492 - acc: 0.0089 - val_loss: 0.0479 - val_acc: 0.0074\n",
      "Epoch 292/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0492 - acc: 0.0084 - val_loss: 0.0484 - val_acc: 0.0065\n",
      "Epoch 293/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0492 - acc: 0.0091 - val_loss: 0.0483 - val_acc: 0.0079\n",
      "Epoch 294/300\n",
      "60000/60000 [==============================] - 6s - loss: 0.0493 - acc: 0.0089 - val_loss: 0.0483 - val_acc: 0.0089\n",
      "Epoch 295/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0492 - acc: 0.0088 - val_loss: 0.0484 - val_acc: 0.0076\n",
      "Epoch 296/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0492 - acc: 0.0078 - val_loss: 0.0483 - val_acc: 0.0075\n",
      "Epoch 297/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0492 - acc: 0.0087 - val_loss: 0.0482 - val_acc: 0.0080\n",
      "Epoch 298/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0492 - acc: 0.0095 - val_loss: 0.0484 - val_acc: 0.0071\n",
      "Epoch 299/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0492 - acc: 0.0085 - val_loss: 0.0485 - val_acc: 0.0066\n",
      "Epoch 300/300\n",
      "60000/60000 [==============================] - 5s - loss: 0.0492 - acc: 0.0085 - val_loss: 0.0484 - val_acc: 0.0087\n",
      "Test loss: 0.0484370409667\n",
      "Test accuracy: 0.0087\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_vec, x_train_vec,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_vec, x_test_vec))\n",
    "score = model.evaluate(x_test_vec, x_test_vec, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データが増えるのでもう少し時間がかかる。ちなみにmnistには各数字が同数ずつ入っているわけではない。<br>誤差を最小にする写像はPCAと関係があるが、PCAは線形な分離しかできない（要確認）。最近の流行は<a href=\"http://cs.stanford.edu/people/karpathy/tsnejs/\">tSNEアルゴリズム</a>である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADqCAYAAAAlBtnSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEAC/j8pSYNk6w6jSAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fa5ed56748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    predicted = model.predict(x_train_vec[i:i+1]).reshape((28,28))\n",
    "    plt.imshow(predicted)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一番強い特徴は全体的な傾きと大きさなのかもしれない。０、３，５，８グループと１ははっきり区別できている。４はまだ苦戦しているものもある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
